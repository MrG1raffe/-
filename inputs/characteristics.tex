\documentclass[../TV&MS.tex]{subfiles}
\begin{document}
    
\section{Характеристики случайных величин}

\subsection{Математическое ожидание}

\emph{Математическое ожидание} (обозначается $\Expec$) обобщает понятие среднего 
арифметического для произвольной случайной величины и показывает, какие значения 
в среднем принимает случайная величина. Оно, как и интеграл Лебега, вводится в 
несколько этапов. В этом определении вероятность $\Pro$ играет роль
Лебеговой меры $\mu$.

\begin{itemize}
 	\item Если $\xi(\omega) = \sum\limits_{i=1}^kx_i \Ind(A_i) \quad (i \not= j 
 	\Rightarrow A_iA_j = \varnothing, \ \bigcup_{i=1}^k A_i = \Omega)$. 
 	
 	$$\Expec \xi = \int\limits_{\Omega} \xi(\omega) \Pro(d\omega) \equiv 
 	\sum\limits_{i=1}^k x_i \Pro(\Set{\omega}{\xi(\omega)=x_i}) =  
 	\sum\limits_{i=1}^k x_i \Pro(A_i).$$

	\item $\xi(\omega) \ge 0$. В этом случае аналогично интегралу Лебега
	$$ \Expec \xi = \int\limits_{\Omega} \xi d\Pro = \lim_{n \to \infty}  
	\left[\sum\limits_{k=1}^{n2^n} \frac{k-1}{2^n} \Pro\left(
	\Set{\omega}{\frac{k-1}{2^n} \le \xi(\omega) < \frac{k}{2^n}}\right) 
	+ n \Pro(\Set{\omega}{\xi(\omega) \ge n})\right].$$
	
	\item Для произвольной $\xi(\omega)$ вводятся
	$$ \xi^+(\omega) = \max\{0, \xi(\omega)\},\quad 
	\xi^-(\omega) = -\min\{0, \xi(\omega)\},$$

	$$ \Expec \xi = \Expec \xi^+ - \Expec \xi^-.$$
\end{itemize}

Вспомним, что любая случайная величина $\xi$ индуцирует вероятностное 
пространство $(\Real, \Bor_\xi, \Pro_\xi)$. Тогда, поскольку $\Pro_\xi(dx)$ есть 
вероятность попасть в $dx$, выразим ее через функцию распределения $\Pro_\xi(dx) 
= F_\xi(x + dx) - F_\xi(x) = dF_\xi(x)$ Тогда перепишем матожидание в более 
привычной форме
$$\Expec\xi = \int\limits_{\Omega} \xi(\omega) \Pro(d\omega) = 
\int\limits_{-\infty}^{+\infty} x \Pro_\xi(dx) = 
\int\limits_{-\infty}^{+\infty} x dF_\xi(x).$$

Перечислим некоторые свойства математического ожидания:
\begin{enumerate}
	\item $\Expec(\xi + a) = \Expec\xi + a \quad \forall a \in \Real$,
	\item $\Expec(a\xi) = a\Expec\xi \quad \forall a \in \Real$,
	\item $\Expec(\xi + \eta) = \Expec\xi + \Expec\eta$ (здесь 
	подразумевается, что существуют два из трех математических ожидания, 
	из чего следует существование третьего).
\end{enumerate}

\begin{Ex}
Рассмотрим дискретную случайную величину $\xi$, которая принимает значения $n$ 
с вероятностью $\frac{c}{n^2} \ (c = \frac{6}{\pi^2})$. По определению
$$ \Expec\xi = \sum\limits_{n=1}^{\infty} n \frac{c}{n^2}.$$

Данный ряд, очевидно, расходится. Ситуацию не спасет даже рассмотрение случайной 
величины $\eta$, принимающей значения $\pm n$ с вероятностью $\frac{c}{2n^2}$, 
которая имеет среднее значение $0$, поскольку $\Expec\eta = 
\sum\limits_{n=1}^{\infty} \frac{c}{2n} - \sum\limits_{n=1}^{\infty} \frac{c}{2n}$, 
что не определено, поскольку интегралы Лебега от $\eta^+$ и $\eta^-$ расходятся.
\end{Ex}

\begin{Ex}
Другим примером является распределение Коши с плотностью $p_\xi(x) = \frac{1}{\pi(1+x^2)}$. 
График этой функции симметричен относительно $0$ и похож на горку, из чего методом 
пристального взгляда можно сделать вывод, что средним значением должно быть $0$. 
Однако $\int\limits_{-\infty}^{+\infty} x dF_\xi(x) =  \int\limits_{-\infty}^{+\infty} 
x p_\xi(x)dx$ расходится, поэтому математического ожидания не существует.
\end{Ex}

\subsection{Моменты, дисперсия, ковариация и прочее}

\begin{Def}
	\mdef{Моментом порядка $k$} случайной величины $\xi$ называется $\Expec\xi^k$.
	\mdef{Абсолютным моментом порядка $k$} случайной величины $\xi$ называется $\Expec|\xi|^k$.
	\mdef{Центральным моментом порядка $k$} случайной величины 
	$\xi$ называется $\Expec(\xi - \Expec\xi)^k$.
\end{Def}

\begin{Def}
	\mdef{Квантилью} случайной величины $\xi$ порядка $q$ называется величина $l_\xi(q)$:
\[
   	l_\xi(q) \colon 
  	\begin{cases}
  		\Pro(\xi \le l_\xi(q)) \ge q \\
  		\Pro(\xi \ge l_\xi(q) \ge 1-q
  	\end{cases}
  \]
  
  В случае $q=\frac12$ квантиль называется \mdef{медианой} и обозначается $\Med\xi$.
  
  Если $q=\frac14$, то $l_\xi$ называется \mdef{квартилью}, если $q=\frac1{10}$, 
  \mdef{децилью}, а если $q=\frac{1}{100}$ --- \mdef{перцентилью}.
\end{Def}

Жизненный смысл медианы заключается в том, что она является точкой, вероятность 
попасть левее которой равна вероятности попасть правее нее. Аналогично можно 
сказать про квантиль любого порядка. Квантиль определена не единственным образом: 
пусть $\xi$ принимает значения $\{0, 1\}$ с вероятностями $\frac12$. Тогда медианой
$\xi$ может быть любая точка из отрезка $[0,1]$.

\begin{Def}
\mdef{Интерквантильный размах} --- величина $R_\xi = l_\xi(\frac34) - l_\xi(\frac14)$ 
--- длина отрезка, вероятность попасть в который равна $\frac12$. 
\end{Def}

С матожиданием и медианой связана задача о <<деловых людях>>. (здесь будет ссылка) 

\begin{Def}
\mdef{Мода} случайной величины $\xi$ --- это наиболее вероятное значение случайной 
величины. Обозначается $\Mod\xi$.
\end{Def}

При наблюдении случайной величины важно знать не только её среднее значение (матожидание), 
но и то, как сильно она от него отклоняется (например, измерение линейкой в среднем дает 
правильный результат, однако необходимо знать погрешность измерения). В связи с этим 
вводится понятие дисперсии.

\begin{Def}
	Пусть для случайной величины $\xi$ существуют конечный $\Expec\xi$ и $\Expec\xi^2$.  
	\mdef{Дисперсией} назывется величина, равная 
	$$ \Disp\xi = \Expec(\xi - \Expec\xi)^2.$$
\end{Def}

	Эту формулу можно привести к более простому для вычисления виду:
$$\Disp\xi = \Expec(\xi^2 - 2\xi\Expec\xi + (\Expec\xi)^2) = \Expec\xi^2 - 
2\Expec\xi\Expec\xi - (\Expec\xi)^2 = \Expec\xi^2 - (\Expec\xi)^2.$$


Перечислим некоторые свойства дисперсии. $\xi,\eta$ - случайные величины, $c \in \Real$.

\begin{enumerate}
	\item $\Disp\xi \ge 0$ как матожидание от неотрицательной функции;
	\item $\Disp c\xi = c^2 \Disp\xi$ --- следует из определения и линейности матожидания;
	\item $\Disp(\xi+c) = \Expec(\xi + c - \Expec(\xi + c))^2 = \Expec(\xi - \Expec\xi)^2 = \Disp\xi$;
	\item $\Pro(\xi = c) = 1 \Leftrightarrow \Disp\xi = 0$ --- отклонение равно нулю для константы;
	\item $\Disp(\xi + \eta) = \Expec(\xi+\eta)^2 - (\Expec(\xi+\eta))^2 = 
	\Expec(\xi^2+2\xi\eta + \eta^2) - (\Expec\xi)^2- 2\Expec\xi\Expec\eta - (\Expec\eta)^2 = 
	\Disp\xi + \Disp\eta + 2(\Expec\xi\eta - \Expec\xi\Expec\eta).$
\end{enumerate}

\begin{Def}
	\mdef{Ковариация} двух случайных величин --- это величина, равная
	$$ \Cov(\xi,\eta) = \Expec(\xi - \Expec\xi)(\eta - \Expec\eta).$$
\end{Def}

	Ковариация положительна, если случайные величины одновременно отклоняются в одну 
	сторону и отрицательная, если в разные. Формулу ковариации так же можно упростить, 
	раскрыв скобки в определении:
	$$\Cov(\xi,\eta) = \Expec\xi\eta - \Expec\xi\Expec\eta.$$

	Таким образом пятое свойство дисперсии можно переписать так:
\begin{itemize}
	\item[5.] $\Disp(\xi \pm \eta) = \Disp\xi + \Disp\eta \pm 2\Cov(\xi,\eta)$.
\end{itemize}

\begin{Def}
	\mdef{Среднеквадратическое отклонение} --- величина $\sigma = \sqrt{\Disp\xi}$.
\end{Def}

    
\subsection{Независимость случайных величин}
\begin{Def}
	Случайные величины $\xi, \eta$ называются \mdef{независимыми}, если 
	 $$ \forall B_1, B_2 \in \Bor \quad \Pro(\xi \in B_1, \eta \in B_2) 
	 = \Pro(\xi \in B_1)\Pro(\eta \in B_2).$$
\end{Def}

\begin{St}
	Для независимых случайных величин $\xi, \eta$ выполнено
	$$\Expec\xi\eta = \Expec\xi\Expec\eta.$$
\end{St}
\begin{Proof}
	Проведем только для случая дискретных случайных величин.
	
	Пусть 
\[
   	\xi = 
  	\begin{cases}
  		x_1, x_2, \ldots, \\
  		p_1, p_2, \ldots,
  	\end{cases}
\]
	то есть $\xi$ приминмает значение $x_i$ с вероятностью $p_i$.
\[
   	\eta = 
  	\begin{cases}
  		y_1, y_2, \ldots,\\
  		q_1, q_2, \ldots.
  	\end{cases}
\]
\begin{multline*}
	\Expec\xi\eta=\sum\limits_{i,j} x_iy_j\Pro(\xi=x_i, \eta=y_j) = 
	\sum\limits_{i,j} x_iy_j\Pro(\xi=x_i)\Pro(\eta=y_j) = \\  
	\sum\limits_{i} x_i\Pro(\xi=x_i)\sum\limits_j y_j\Pro(\eta=y_j) = 
	\Expec\xi\Expec\eta.
\end{multline*}
\end{Proof}

Для независимых случайных величин
$$\Cov(\xi,\eta) = \Expec\xi\eta - \Expec\xi\Expec\eta = 0.$$

Обратное, вообще говоря, неверно: пусть мы равновероятно выбираем 
одну из точек $(-1, 0), (0, 1), (1, 0), (0, -1)$. Каждая координата 
принимает значения $-1, 0, 1$, но координаты зависимы, так как 
$\Pro(x=0,  y=0) = 0$ (никогда не выбираем $(0,0)$), а 
$\Pro(x=0)\Pro(y=0) = \frac12\frac12 \not=0$. Однако
$$\Expec x = \Expec y = \Expec xy = 0$$ 

\noindent
в силу симметрии задачи. Отсюда $\Cov(x,y) = 0$.

Таким образом, ковариация показывает зависимость величин, 
однако не дает представления, насколько они зависимы. Для это вводится 
понятие коэффициента корреляции --- нормированная ковариация.

\begin{Def}
\mdef{Коэффициент корреляции} --- величина, описываемая формулой
$$\rho(\xi, \eta) = \frac{\Cov(\xi, \eta)}{\sqrt{\Disp\xi\Disp\eta}}.$$
\end{Def}

Некоторые свойства коэффициента корреляции:
\begin{enumerate}
	\item $|\rho(\xi, \eta)| \le 1$ --- неравенство Коши-Буняковского.
	\item $|\rho(\xi, \eta)| = 1 \Leftrightarrow \exists a, b \colon 
	\quad \Pro(\xi = a\eta + b) = 1$ --- равенство достигается, 
	если случайные величины линейно зависимы.
	
	\item Для независимых случайных величин $\rho(\xi, \eta) = 0$.
\end{enumerate} 

Некоторые пояснения к первому свойству.
Ковариация является псевдоскалярным произведением, то есть 
выполнены все аксиомы скалярного произведения, кроме половины четвертой. 
Поэтому для нее выполнено неравенство Коши-Буняковского:
\begin{equation}
    \Cov(\xi, \eta)^2 \le \Cov(\xi, \xi)\Cov(\eta, \eta)
.\end{equation} 
Теперь поделим на произведение ковариаций, извлечем корень и получим нужное свойство.

\subsection{Неравенство Маркова, неравенство Чебышева}
\begin{Lem}
Для любой неотрицательной неубывающей функции $g(x)$ выполнено неравенство 
$$\Pro(|\xi| > x) \leqslant \frac{\Expec g(|\xi|)}{g(x)}$$
\end{Lem}
\begin{Proof} \\
\begin{multline*}
    \Expec g(|\xi|) = \Expec g(|\xi|) \Ind(|\xi| \geqslant x) +
    \Expec g(|\xi|) \Ind(|\xi| < x) \geqslant \\
    \Expec g(|\xi|) \Ind(|\xi| \geqslant x) \geqslant 
    g(x)\Expec\Ind(|\xi| \geqslant x) = g(x)\Pro(|\xi| > x)
\end{multline*}
Здесь мы воспользовались представлением $1 = \Ind(A) + \Ind(\overline{A})$, 
затем неотрицательностью функции и, следовательно, ее матожидания. 
Далее использовалась монотонность функции, и в последнем переходе тождество 
$\Expec \Ind(A) = 1 \Pro(A) + 0 \Pro(\overline{A}) = \Pro(A).$
\end{Proof}

Из этой леммы следуют два полезных неравенства.

\begin{Th} [неравенство Маркова]
Для любой случайной величины $\xi$, имееющей конечное $\Expec|\xi|$, выполнено
$$\Pro(|\xi| > \varepsilon) \le \frac{\Expec|\xi|}{\varepsilon}$$
\end{Th}
\begin{Proof}\\
В неравенстве леммы возьмем $g(x) = x$.
\end{Proof}

\begin{Th} [неравенство Чебышева]
Для любой случайной величины $\xi$, имееющей конечный первый и второй момент, выполнено
$$\Pro(|\xi - \Expec\xi| > \varepsilon) \le \frac{\Disp\xi}{\varepsilon^2}$$
\end{Th}
\begin{Proof}\\
В неравенстве леммы возьмем $g(x) = x^2$.
\end{Proof}


\newpage
\end{document}
