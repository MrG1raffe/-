\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bbm} % для индикатора
\usepackage{chngcntr} % без этого пакета у меня не робит counterwithin
\usepackage{ulem} % нужен для \uline, которая умеет переносить подчеркнутый текст
\usepackage{cancel}
\usepackage{tikz}  
\usepackage{pb-diagram}
\usepackage{pgfplots}
\usepackage{caption}
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,positioning,fit,petri}
\usepgflibrary{arrows.meta}
\usetikzlibrary{arrows.meta}
\graphicspath{{pictures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\graphicspath{{pictures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\textheight=24cm
\textwidth=16cm
\oddsidemargin=0pt
\topmargin=-1.5cm
\parindent=24pt
\parskip=0pt
\tolerance=2000
\flushbottom

\newtheorem{Th}{Теорема}
\newtheorem{Def}{Определение}
\newtheorem{Lem}{Лемма}
\newtheorem{St}{Утверждение}

\newenvironment{Proof}{\par\noindent{\bf Доказательство}}{$\blacksquare$} 
\newenvironment{Ex}{{\bf Пример}\ }{}
\newenvironment{Wtf}{\includegraphics[height=5mm]{ping}}{}
\newenvironment{Why}{\includegraphics[height= 5mm]{cat}}{}
\newenvironment{Note}{\par\noindent{\bf Замечание}}{}

\numberwithin{Th}{section}
\numberwithin{Def}{section}
\numberwithin{Lem}{section}
\numberwithin{St}{section}
\numberwithin{equation}{section}
\counterwithin*{section}{part}

\newcommand\Set[2]{\left\{ #1 \colon #2 \right\}}
\newcommand\Sum[3]{\sum\limits_{#1 = #2}^{#3}}
\newcommand\Pro{\mathbb{P}} % вероятность
\newcommand\Ev{\mathscr{A}} % алгебра событий
\newcommand\Bor{\mathscr{B}} % борелевская сигма-алгебра
\newcommand\Real{\mathbb{R}} % вещественная прямая
\newcommand\Int{\mathbb{Z}} % целые числа
\newcommand\Nat{\mathbb{N}} % натуральные числа
\newcommand\Expec{\mathbb{E}} % матожидание 
\newcommand\Disp{\mathbb{D}}  % дисперсия
\newcommand\Ind{\mathbbm{1}} % индикатор
\newcommand\Med{{\rm med \,}} % медиана
\newcommand\Cov{{\rm cov \,}} % ковариация
\newcommand\Mod{{\rm mod \,}} % мода
\newcommand\Pois{{\rm Pois \,}} 
\newcommand\Norm{\mathcal{N}} % нормальное распределение
\newcommand\xequiv[1]{\stackrel{\mathrm{ #1}}{\equiv}}

\begin{document}

\tableofcontents
\newpage

\part{Теория вероятности}
\textit{Ты спросишь меня, кого я люблю больше: тебя или теорию вероятности. Я отвечу, что почти наверное тебя и ты уйдешь, так и не узнав, что почти наверное значит с вероятностью 1.}

\newpage

\section{Вероятностное пространство}
\qquad Методы теории вероятности работают в ситуациях, называемых стохастическими. Для них характерны три свойства:
\begin{enumerate}
	\item Непредсказуемость 
	\item Воспроизводимость 
	\item Устойчивость частот
\end{enumerate}

Для описания стохастических ситуаций ситуаций необходимо определить функцию вероятности. Её область определения назывется множеством событий.
В свою очередь событие (такое как, например, выпадание чётного числа на кубике) могут являться совокупностью неких более простых событий, описывающих стохастическую ситуацию (число, выпавшее на кубике). Последнее множество называется множетсвом элементарных исходов и обозначается $\Omega$.  

Множество событий, обозначаемое $\Ev$, должно обладать следующими интуитивными свойствами:
\begin{enumerate}
	\item Отрицание события есть событие (если <<пойдет дождь>> событие, то <<не пойдет дождь>> также событие)
	\item Объединение событий есть событие (<<пойдет дождь>> или <<пойдет снег>>)
	\item Все множество элементарных исходов является событие (<<Что-нибудь да произойдет>>)
\end{enumerate}

Формализуя эти свойства, получаем определение алгебры.
\begin{Def}
Семейство $\Ev$ подмножеств множества $\Omega$  называется \\ \underline{алгеброй}, если 
\begin{enumerate}
	\item $\forall A \in \Ev, B\in \Ev \Rightarrow A \bigcup B \in \Ev$
	\item $\forall A \in \Ev \Rightarrow \overline{A} \in \Ev$
	\item $\Omega \in \Ev$
\end{enumerate}
\end{Def}

Из аксиом алгебры и формулы $A\bigcap B = \overline{\overline{A} \bigcup \overline {B}}$ следует, что пересечений событий явялется событием.

Наименьшей возможной алгеброй является $\left\{ \Omega, \varnothing \right\}$

\begin{Def}
Семейство $\Ev$ подмножеств множества $\Omega$  называется \\ \underline{$\sigma$-алгеброй}, если 
\begin{enumerate}
	\item $\forall A_1, \dots, A_n, \ldots \in \Ev \Rightarrow \bigcup\limits_{i=1}^{\infty} A_i \in \Ev$
	\item $\forall A \in \Ev \Rightarrow \overline{A} \in \Ev$
	\item $\Omega \in \Ev$
\end{enumerate}
\end{Def}

\begin{Def}
Пусть $\mathscr{K}$ - класс подмножеств $\Omega$. $\sigma$-алгебра $\sigma (\mathscr{K})$,\\ \underline{ порожденная классом $\mathscr{K}$} --- наименьшая $\sigma$-алгебра, 
содержащая $\mathscr{K}$, то есть любая $\sigma$-алгебра, содержащая $\mathscr{K}$, содержит и $\sigma (\mathscr{K})$.
\end{Def}
\begin{Ex}
$\sigma$-алгеброй, порожденной $\mathscr{K} = A$, будет являться $\sigma(A) = \left\{ \varnothing, A, \overline(A), \Omega \right\}$.
\end{Ex}

$\sigma$-алгебра является более узким понятием, нежели алгебра, то есть любая $\sigma$-алгебра является алгеброй, а обратное, вообще говоря, неверно. \\
\begin{Ex}
Пусть $\Omega = \mathbb{R},\  \Ev$ содержит конечные подмножества $\Omega$ и их дополнения. Для такого множества выполнены все аксиомы алгебры: 
$\Omega = \overline{\varnothing} \in \Ev$, объединение конечных множеств есть конечное множество, объединение конечного множества с дополнением к конечному 
множеству так же является дополнением к некоторому множеству. То же можно сказать и об объединении двух дополнений. Таким образом, $\Ev$ является алгеброй.
Все элементы $\Ev$ либо конечны, либо континуальны, поэтому $\Ev$ не содержит $\mathbb{N}$. Но $\mathbb{N} = \bigcup\limits_{i=1}^{\infty}\{i\}$, то есть
не выполнено свойство счетной аддитивности из определения $\sigma$-алгебры.
\end{Ex}

\begin{Def}
Пара $(\Omega, \Ev)$ называется \underline{измеримым пространством}, если $\Ev$ является $\sigma$-алгеброй. Если же $\Ev$ - алгебра, то  $(\Omega, \Ev)$ --- \underline{измеримое пространство} \underline{ в широком смысле}.
\end{Def}

\begin{Def}
\underline{Вероятностью} называется функция $\Pro \colon \Ev\rightarrow \mathscr{R}_+$, удовлетворяющая свойстам
\begin{enumerate}
	\item $\forall A \in \Ev \quad \Pro (A) \ge 0$
	\item $\forall A_1, \dots, A_n, \ldots \in \Ev \quad A_i \bigcap A_j  = \varnothing\  (i \not= j)  \Rightarrow \Pro (\bigcup\limits_{i=1}^{\infty} A_i) = \sum\limits_{i = 1}^{\infty}\Pro( A_i)$
	\item $\Pro (\Omega) = 1$
\end{enumerate}
\end{Def}

\begin{Def}
\underline{Вероятностным пространством} $(\Omega, \Ev, \Pro)$ называется измеримое пространство $(\Omega, \Ev)$, снабженное вероятностью $\Pro$.
\end{Def}
\begin{Wtf}
Кому вообще нужна $\sigma$-алгебра событий, и зачем весь этот огород, если можно рассматривать множество всех подмножеств множества $\Omega$? Когда-то кто-то доказал, что в случае очень большого множества элементарных исходов, например, континуального, множество $2^{\Omega}$ будет иметь такую крокодильски большую мощность, что вся теория сломается. Таким образом, алгебры нужны для того, чтобы вероятность имела хорошую область определения.
\end{Wtf}

\paragraph{Свойства вероятности}
\begin{enumerate}
	\item $\Pro (\varnothing) = 0$ 
	\item $\Pro (\overline{A}) = 1 - \Pro (A)$
	\item $A \subseteq B \quad \Rightarrow \Pro (A) \le \Pro (B)$
	\item $\Pro (A) \le 1$
	\item $\Pro (A \bigcup B) = \Pro (A) + \Pro (B) - \Pro (AB)$ 
	\item $\Pro (A \bigcup B) \le \Pro (A) + \Pro (B)$ 
	\item $\Pro (\bigcup\limits_{i=1}^{n} A_i) = \sum\limits_{k=1}^{n} \sum\limits_{i_1<\dots <i_k} (-1)^{k+1} \Pro(A_{i_1}A_{i_2}\ldots A_{i_k})$
	\item $\Pro (\bigcap\limits_{i=1}^{n} A_i) \ge 1 - \sum\limits_{i=1}^{n} \Pro (\overline{A_i})$ - неравенство Бонферрони
\end{enumerate}

Второй пункт в определении вероятностной меры нельзя заменить аналогичным с конечными объединением и суммой. Однако если добавить к данному требованию так 
называемое свойство непрерывности вероятностной меры, т.е $$\forall B_1, B_2, \ldots \in \Ev \quad B_{n+1} \subseteq B_n \Rightarrow \lim_{n \to \infty} \Pro(B_n) = \Pro(B)$$, то они вместе будут эквивалентны 2. из определения вероятности.

\begin{St}
$\forall A_1, \dots, A_n, \ldots \in \Ev \quad A_i \bigcap A_j  = \varnothing\  (i \not= j)  \Rightarrow \Pro (\bigcup\limits_{i=1}^{\infty} A_i) = \sum\limits_{i = 1}^{\infty} A_i
\Leftrightarrow ( \forall A_1, \dots, A_n \in \Ev \quad A_i \bigcap A_j  = \varnothing\  (i \not= j)  \Rightarrow \Pro (\bigcup\limits_{i=1}^{n} A_i) = \sum\limits_{i = 1}^{n} A_i)  \land  (\forall B_1, B_2, \ldots \in \Ev \quad B_{n+1} \subseteq B_n \Rightarrow \lim\limits_{n \to \infty} \Pro(B_n) = \Pro(B))$
\end{St}
\begin{Proof}
\\ $\Rightarrow$\\
Обозначим $C_n = B_n \setminus B_n+1$. Множества $B, C_1, C_2, \ldots$ не имеют общих точек.\\
$\forall n \quad B_n =  \bigcup\limits_{k=n}^{\infty} C_k \bigcup B$. Тогда $\Pro(B_1) = \Pro(B) + \sum\limits_{k=1}^{\infty} \Pro(C_k)$. Отсюда следует, что ряд в правой части сходится, так как имеет конечную сумму.
$\Pro(B_n) = \Pro(B) + \sum\limits_{k=n}^{\infty} \Pro(C_k)$. При $n \to \infty$ сумма ряда стремится к нулю как остаточный член ряда из предыдущего выражения.
В предельном переходе получаем свойство непрерывности.
\\ $\Leftarrow$\\
Рассмотрим произвольный набор $A_1, A_2, \ldots \in \Ev \quad A_iA_j = \varnothing$.\\
$\Pro(\bigcup\limits_{i=1}^{\infty} A_i) = \Pro(\bigcup\limits_{i=1}^{n} A_i) + \Pro(\bigcup\limits_{i=n + 1}^{\infty} A_i) =\sum\limits_{i = 1}^{n} \Pro(A_i) +  
\Pro(\bigcup\limits_{i=n + 1}^{\infty} A_i) $.\\
 Обозначим $B_n = \bigcup\limits_{i=n + 1}^{\infty} A_i,\quad B_{n+1} \subseteq B_n \quad \forall n,\quad \bigcap\limits_{n=1}^{\infty} B_n= \varnothing$ \\
$\sum\limits_{i=1}^{\infty} \Pro(A_i) = \lim\limits_{n \to \infty} (\Pro(\bigcup\limits_{i=1}^{\infty} A_i) - \Pro(B_n)) = \Pro(\bigcup\limits_{i=1}^{\infty} A_i) - \lim\limits_{n \to \infty} \Pro(B_n) = \Pro(\bigcup\limits_{i=1}^{\infty} A_i)$
\end{Proof}

\begin{Th}[Каратеодори]
Пусть $(\Omega, \Ev)$ --- измеримое пространство в широком смысле, а некоторая функция $\Pro$ обладает свойствами вероятностной меры.Тогда на измеримом пространстве
$(\Omega, \sigma(\Ev))\  \exists !\  \Pro' \colon \forall A \in \Ev \quad \Pro(A) = \Pro'(A)$
\end{Th}
\begin{Proof}
отсутсвует
\end{Proof}\\\\
\begin{Why}
Зачем это нужно? Теорема Каратеодори говорит о том, что любую вероятностную меру, заданную на алгебре, можно однозначно продолжить на $\sigma$-алгебру,
то есть расширить область ее определения. При этом значения функции на алгебре не изменятся. Теорема будет использоваться при определении интеграла Лебега.
\end{Why}
\newpage


\section{Условная вероятность. Независимость событий}

\qquadРассмотрим произвольное $B \in \Ev \colon \quad \Pro(B) > 0$.
\begin{Def}
\underline{Условной вероятностью} события $A \in \Ev$ при условии $B$ называется $\frac{\Pro(AB)}{\Pro(B)} =\colon \Pro(A|B) = \Pro_B(A)$
\end{Def}

Что это означает на пальцах? Условная вероятность $\Pro(A|B)$ --- это веротяность того, что произойдет событие $A$, если мы точно знаем, что произошло событие $B$.\\

\parbox[b][3 cm][t]{20mm}{\includegraphics[height=30mm]{cond_prob}}
\hfill
\parbox[b][3 cm][t]{100mm}{
	Графически это означает, что, когда произошло событие $B$, мы оказались в круге $B$. Тогда формула  $\frac{\Pro(AB)}{\Pro(B)}$ есть просто вероятность попасть в $AB$.
}\\

Из определения следует так называемый <<Закон умножения вероятностей>>:
$$\Pro(A|B)\Pro(B)=\Pro(AB)$$

Легко проверяется, что $(B, \Ev_B, \Pro_B)$, где $\Ev_B = \Set{A \bigcap B}{A \in \Ev}$, так же является вероятностным пространством. \\
\begin{Wtf}
Зачем нужно требование $\Pro(B) > 0$, если можно в случае $\Pro(B) = 0$ доопределить условную вероятность нулем как вероятность при условии невозможного события?
При таком доопределении нарушится аксиома 3. вероятности $\Pro_B$, поскольку $\Pro_B(B)$ по доопределению будет равно $0$.
\end{Wtf}

\begin{Def}
События $A, B \in \Ev$ называются \underline{независимыми}, если $$\Pro(AB) = \Pro(A) \Pro(B)$$.
\end{Def}

Для независимых событий $$\Pro(A|B) = \frac{\Pro(A)\Pro(B)}{\Pro(B)} = \Pro(A)$$.\\
\begin{Ex}
Являются ли несовместные события ($AB = \varnothing$) независимыми? Нет, пусть  $A, B \in \Ev \colon \quad \Pro(A) > 0, \ \Pro(B) > 0$. Тогда $\Pro(AB) = \Pro(A)\Pro(B) = 0$, 
что является противоречием. По-простому, если произошло одно из несовместных событий, то второе уже не может произойти, и его условная веротяность равна 0, а не
вероятности самого события, что требуется для независимости.
\end{Ex}

Следующее определение обобщает понятие независимости на произвольное количество событий.
\begin{Def}
События $A_1, A_2, \dots, A_n$ называются \underline{независимыми в совокупности}, если 
$$\forall m = 2, \dots, n \quad \forall 1 \le j_1 < \ldots < j_m \le n \quad 
\Pro(\bigcap_{k=1}^{m}A_{j_k})=\prod_{k=1}^{m} \Pro(A_{j_k})$$
\end{Def}
\begin{Ex}
На примере тетраэдра Бернштейна можно убедиться в том, что попарной независимости событий недостаточно для независимости в совокупности. Рассмотри тетраэдр, у 
которого три стороны покрашены в красный, синий и зеленый, а четвертая содержит все три цвета. События \{выпадет красный\}=\{К\}, \{выпадет синий\}=\{С\}, \{выпадет зеленый\}=\{З\}
попарно независимы (например, вероятность события \{С\}$\bigcap$\{К\} равна веротяности выпадения четвертой грани, т. е. $\frac{1}{4}$, в то время как выпадения 
каждого цвета равна $\frac12$). Однако $\Pro$(\{С\}$\bigcap$ \{К\}$\bigcap$ \{З\}) = $\frac14 \not= (\frac12)^3$.
\end{Ex}
\newpage

\section{Формула полной веротяности. Формула Байеса}

\begin{Def}
$B_1, \ldots, B_n$ образуют \underline{полную группу}, если выполнены следующие условия:
	\begin{enumerate}
		\item $\Pro(B_i) > 0 \quad \forall i = 1, \ldots, n$
		\item $B_iB_j = \varnothing \quad (i \not= j)$
		\item $\bigcup\limits_{i=1}^nB_i = \Omega$
	\end{enumerate}
\end{Def}

\begin{Th}
Пусть $B_1, \ldots, B_n$ образуют полную группу. Вероятность события $A \in \Ev$ можно вычислить по \underline{формуле полной вероятности}:
$$\Pro(A) = \sum\limits_{i=1}^{n} \Pro(A|B_i)\Pro(B_i)$$
\end{Th}

\begin{Proof}
\\
$A=\bigcup\limits_{i=1}^{n}AB_i, \quad AB_i \bigcap AB_j = \varnothing \quad (i \not= j)$ \\
$\Pro(A) = \sum\limits_{i=1}^n \Pro(AB_i) = \sum\limits_{i=1}^{n} \Pro(A|B_i)\Pro(B_i)$ \\
Последний переход следует из  закона умножения вероятностей.
\end{Proof}
\\

Первое требование определения полной группы необходимо для возможности определить условную веротяность, второе позволяет 
разбить множество $A$ на непересекающиеся части. Третье требование, вообще говоря, можно ослабить, потребовав, чтобы $A \subseteq \bigcup\limits_{i=1}^nB_i$.
Доказательство при этом не изменится. \\
\begin{Ex}
Проиллюстрировать формулу полной вероятности можно обычным экзаменом: $A$ - \{студент сдал экзамен\}, $B_i$ - \{студент попал к преподавателю $i$\}.
Как и в любой другой лотерее, можно оценить вероятность попадания к преподавателю $i$, то есть $\Pro(B_i)$, а трезво оценивая свои силы можно прикинуть 
и вероятность сдать тому или иному преподавателю $\Pro(A|B_i)$. Зная все вышеперечисленое, несложно по формуле вычислить вероятность успешной сдачи.
\end{Ex}
\\

Формула полной веротяности используется для вычисления априорной вероятности, т.е. вероятности события, которое  еще не произошло.
Пусть теперь \\$\Pro(A) > 0$. Тогда $\Pro(B_i|A)=\frac{\Pro(AB_i)}{\Pro(A)}$. Используя формулу полной вероятности, получаем \underline{формулу Байеса}:
$$\Pro(B_i|A)=\frac{\Pro(A|B_i)\Pro(B_i)}{\sum\limits_{j=1}^n\Pro(A|B_j)\Pro(B_j)}$$

Формула Байеса используется для вычисления апостериорной вероятности. То есть уже известно, что произошло некоторое событие $A$, и нужно вычислить вероятность
того, что произошло некоторое $B_i$. В примере с экзаменом, например, может быть известно, что студент не сдал экзамен, и хочется вычислить вероятность того, что он
сдавал преподавателю <<Р>>.

\newpage
\section{Случайные величины}

\qquad Случайные события "--- это хорошо, но с события типа <<на монетке выпал герб>> плохо формализуемы, а мы хотим формальности и математичности. Поэтому вместо всяких событий хочется работать с числами. Вот этим и займемся. При рассмотрении случайных событий мы ввели вероятностное пространство, которе выглядит так:
$$(\Omega, \Ev, \Pro),$$
где $\Omega$ "--- множество элементарных событий, $\Ev$ "--- $\sigma$-алгебра подмножеств множества элементарных событий, а $\Pro$ "--- вероятность. Мы же будем рассамтривать теперь тройку
$$(\Real, \Bor, \Pro),$$
где $\Real$ "--- действительная прямая, $\Bor$ "--- борелевская $\sigma$-алгебра, а $\Pro$ "--- вероятность. Поясним.

\begin{Def}
\underline{Борелевской $\sigma$-алгеброй} называется минимальная $\sigma$-алгебра, содержащая все открытые подмножества топологического пространства. Элементы борелевской $\sigma$-алгебры называются \underline{борелевскими множествами}.
\end{Def}
\begin{Wtf}
Мы будем рассматривать только топологическое пространство $\Real$, так что это стремное словосочетание можно прямо 
сейчас забыть и понимать открытое множество как открытое множество из матана (все точки внутренние).
\end{Wtf}\\
\begin{Ex}
Покажем, что все <<хорошие>> множества являются Борелевскими.
\begin{enumerate}
\item Все открытые интревалы входят по определению.
\item Отрезок вида $[a, b]$ входит как $\overline{(-\infty, a) \bigcup (b, +\infty)}$.
\item Точка ходит как вырожденный отрезок $[a, a]$.
\item Счетное объединение таких множеств входит по поределению.
\end{enumerate}
\end{Ex}

Теперь формально введем понятие случайной величины (может использоваться сокращение с.в.).

\begin{Def}
Пусть $(\Omega, \Ev, \Pro)$ "--- вероятностное пространство.\\ Тогда \underline{случайной величиной $\xi$} называется функция $\xi : \Omega \to \Real$, измеримая относительно $\Ev$ и $\Bor$. По-другому, $\xi$ "--- случайная величина, если
$$\forall B \in \Bor \quad \xi^{-1}(B) = \lbrace \omega : \xi(\omega) \in B \rbrace \in \Ev$$.
\end{Def}
\begin{Wtf}
Таким финтом ушами мы, по сути, сопоставили каждому событию какое-то <<хорошее>> множество на числовой прямой, и можем рассматривать не вероятности событий, а вероятности попадания в эти <<хорошие>> подмножества числовой прямой.
\end{Wtf}

Введем еще несколько бесполезных определений, которые в дальнейшем использоваться не будут, но знать их не вредно.

\begin{Def}
С каждой случайной величиной свяжем два вероятностных пространства: первое --- $(\Omega, \Ev_\xi, \Pro)$ --- вероятностное пространство, \underline{порожденное $\xi$}. Здесь 
$\Ev_\xi$ - наименьшая $\sigma$-алгебра, для которой выполняется свойство измеримости. Второе --- $(\Real, \Bor, \Pro_\xi)$, где $\Pro_\xi(B) = \Pro(\xi^{-1}(B)) \quad \forall B \in \Bor$ и называется \underline{распределением вероятностей $\xi$}.
\end{Def}

Идем дальше в~сторону упрощения работы со случайностями. Вместо того чтобы рассматривать произвольные борелевские множества, мы будем рассамтривать только множества вида $(-\infty, x)$. Действительно, интервал $(a, b)$ получается из~полупрямых так: $(a, b) = (-\infty, b) \setminus (-\infty, a]$  Таким образом, мы можем рассматривать случайные величины только на таких множествах. Здесь имеется в виду, что для удовлетворения определению случайной величины достаточно измеримости только на
 полупрямых, что следует из следующих свойств полного прообраза: прообраз объединения есть объединение прообразов, прообраз пересечения есть пересечение прообразов,
 прообраз отрицания есть отрицание прообраза. Выше показано, что из полупрямых с помощью этих операций можно получить интервалы, а из интервалов и все $\Bor$.

Теперь несколько полезных утверждений. Пусть $\xi$ --- случайная величина. Тогда $-\xi$ также случайная величина, так как её прообраз от любой полупрямой является
прообразом $\xi$ от симметричной полупрямой, то есть лежит в $\Ev$. $\xi + c$ также будет случайной величиной, поскольку ее прообразом для любой полупрямой будет
прообраз $\xi$ для полупрямой, сдвинутой на $c$, то есть будет лежать в $\Ev$.

\begin{St}
Пусть $\xi, \eta$ --- случайные величины. Тогда множество $\Set{\omega}{\xi(\omega) < \eta(\omega)}$ является событием.
\end{St}
\begin{Proof}
$\Set{\omega}{\xi(\omega) < И\eta(\omega)} = \bigcup\limits_{r \in \mathbb{Q}}\Set{\omega}{\xi(\omega) < r, \eta(\omega) > r}$. 
Заметим, что $\Set{\omega}{\xi(\omega) < r)}$ является событием. Аналогично для $\eta$. Выражение, написанное выше, является счетным объединением пересечений двух событий, то есть событием.
\end{Proof}

Похожими махинациями, а также с использованием этого утверждения, доказывается, что $\xi^2, \xi + \eta, \xi\eta$ являются случайными величинами.
Более того, если $\xi_1, \ldots, \xi_n$ --- с.в., а функция $\phi(x_1, \ldots, x_n)$ является непрерывной на множестве их значений, то $\phi(\xi_1, \ldots, \xi_n)$ будет случайной 
величиной. Это не доказывалось.

\begin{Def}
Рассмотрим вероятностное пространство $(\Omega, \Ev, \Pro)$ и определенную на нем случайную величину $\xi$. Тогда её \underline{функцией распределения $F_\xi(x)$}
 называется функция $F_\xi : \Real \to \Real$
$$F_\xi(x) = \Pro(\omega : \xi(\omega) < x) = \Pro(\xi < x)$$.
\end{Def}

Запись $\Pro(\xi < x)$ является в некотором смысле жаргонной, так как аргументов вероятности должно быть событие из $\Ev$. Но $\xi < x$ мы в дальнейшем будем 
отождествлять с оъединением элементарных событий, образ которых меньше $x$. Из определения случайной величины получаем, что это объединение является событием,
поэтому применение к нему функции вероятности корректно.

Функция распределения (сокращение ф.р.) является очень полезной штукой, поскольку имеет достаточно простой вид и несет в себе всю информацию о распределении, то есть однозначно 
определяет $\Pro_\xi$.

Рассмотрим основные свойства функции распределения:
\begin{enumerate}
	\item $F_\xi(x) \in [0, 1]$
	\item $\lim\limits_{x \to -\infty} F_\xi(x) = 0$
	\item $\lim\limits_{x \to +\infty} F_\xi(x) = 1$
	\item $F_\xi(x)$ монотонно не убывает.
	\item $F_\xi(x)$ непрерывна слева.
\end{enumerate}

Вероятность попадания с.в. в полуинтервал $\Pro_\xi[a,b) = F_\xi(b) - F_\xi(a)$. При стремлении $b \to a$ получим $\Pro(\xi = a) = F_\xi(a+) - F_\xi(a)$, то есть 
вероятность попадания в точку равна скачку функции распределения в этой точке.

\begin{Def}
Точка $x_0$ называется \underline{точкой роста} $F_\xi(x)$, если $\forall \epsilon > 0 \quad \Pro(x_0 - \epsilon \le \xi < x_0 + \epsilon) > 0$
\end{Def}
\begin{Ex}
Это очень полезный пример, который будет использоваться в матстате и который очень любят спрашивать. Пусть $\xi$ --- случайная величина. $\eta = F_\xi(\xi)$. Чему равна
$F_\eta(x)$? По определению $F_\eta(x) = \Pro(\eta < x) = \Pro(F_\xi(\xi) < x)=\Pro(\xi < F_\xi^{-1}(x)) = F_\xi(F_\xi^{-1}(x))=x$. Вообще, тут было бы неплохо 
сказать, что $F_\xi$ непрерывна и строго монотонна, чтобы со спокойной совестью использовать обратную функцию. Таким образом $\eta$ имеет равномерное распределение.
\end{Ex}

\newpage

\section{Понятие меры и интеграла Лебега}

\qquad Мера Лебега вводится потихонечку: сначала для полуинтервала, потом для борелевских множеств, а затем и для всей числовой прямой. Основная идея --- обобщение
понятия длины на страшные, ненормальные множества.

Вспомним сначала аксиомы меры $\mu$ из функана (хех): это функция на множестве $B$ (в нашем случае $[0, 1)$), $\mu \colon B \to \Real$ со свойствами
\begin{enumerate}
	\item $\forall A \in B \quad \mu(A) \ge 0$
	\item $\forall A_1, A_2, \ldots \in B \colon A_iA_j = \varnothing \quad (i \not= j) \Rightarrow \mu(\bigcup\limits_{i=1}^\infty A_i) = \sum\limits_{i=1}^\infty \mu(A_i)$
\end{enumerate} 

Введем сначала меру на борелевских множествах полуинтервала $[0, 1)$, то есть на $\Bor_{[0, 1)}$. Итак, 
\begin{itemize}
	\item На полуинтервалах $(a, b)$ введем меру как $\mu_{[0, 1)}((a, b)) = b - a$
	\item Тогда мера одной точки равна нулю, и мы можем не обращать внимание на концы множеств
	\item Любое конечное объединение, пересечение, отрицание таких интервалов есть конечное объединение интервалов (и, возможно, конечное число точек-концов)
	На них введем меру как сумму мер этих интервалов.
	\item Теперь осталось определить меру на бесконечных объединениях/пересечениях. Для этого воспользуемся теоремой Каратеодори, согласно которой можно продолжить
	меру на $\sigma$-алгебре.
\end{itemize}

Аналогичным образом определим меру $\mu_{[k, k+1)}$ на всех полуинтервалах вида $[k, k+1) \quad (k \in \mathbb{Z})$.
Осталось доопределить меру на всей $\Bor$. $\forall A \in \Bor \quad$
 $$\mu(A)=\sum\limits_{i=-\infty}^{\infty}\mu_{[i, i+1)}(A \bigcap [i, i+1))$$
 
 Теперь введем интеграл \underline{интеграл Лебега} (интеграл по мере Лебега). В отличие от интеграла Римана, где происходит разбиение области определения и выбирается
 значение функции из образа элемента разбиения, в интеграле Лебега разбивается область значений (то есть $Oy$), и некоторое значение из элемента разбиения умножается на
 меру прообраза этого элемента, затем все благополучно складывается. Введем теперь это формально.
 
 Будем рассматривать функции $f \colon \Real \to \Real$ такие, что $\forall c \in \Real \quad \Set{x}{f(x) < c} \in \Bor$, то есть прообразы полупрямых являются
  измеримыми множествами. А как было показано выше, отсюда следует, что прообразы всех борелевских множеств являются измеримыми.
  
  Введем сначала понятие \underline{индикатора}. Индикатор события $A$ - это случайная величина, принимающая значение 1, если событие произошло, и 0 в противном случае.
  Таким образом
  \[
  	\Ind(x) = 
  	\begin{cases}
  		1, x \in A \\
  		0, x \not\in A
  	\end{cases}
  \]
  
 Определим интеграл Лебега на полуинтервале $[0, 1)$
\begin{itemize}
 	\item Пусть функция имеет вид $f(x) = \sum\limits_{i=1}^ky_i \Ind(A_i) \quad (i \not= j \Rightarrow A_iA_j = \varnothing, \ \bigcup_{i=1}^k A_i= [0,1))$. Функции такого вида 		называются \underline{финитными}.  В этом случае
 		$$\int\limits_0^1f(x)\mu(dx) = \sum\limits_{i=1}^k y_i \mu(A_i) $$
	То есть берется мера кусочка, на котором функция принимает значение $y_i$, и умножается на это значение. Получается нечто, напоминающее площадь под графиком.
	\item Пусть теперь $f(x) \ge 0$ на $[0, 1)$. Будем приближать функцию финитной. Возьмем отрезок области значений $[0, n], n \in \mathbb{N}$. Разобьем этот отрезок на
	$n2^n$ кусочков $[\frac{k-1}{2^n}, \frac{k}{2^n}], k = 1, 2, \dots, n2^n$. На каждом кусочке скажем, что значение функции равно $\frac{k-1}{2^n}$. Осталось приблизить 
	$[n, +\infty)$. Будем считать это одной частью, на которой функция принимает значение $n$. Осталось устремить n к бесконечности: тогда, так как размер каждого кусочка первой части равен $\frac{1}{2^n}$, их длина станет бесконечно малой, а та часть, которую мы <<обрубили>> сверху (то есть $[n, +\infty)$), уйдет в бесконечность. Итак,
	$$ \int\limits_0^1 f(x) \mu(dx) = \lim_{n \to \infty}  \biggl[  \sum\limits_{k=1}^{n2^n} \frac{k-1}{2^n} \mu\biggl(\Set{x}{\frac{k-1}{2^n} \le f(x) < \frac{k}{2^n}}\biggl) 
	+ n \mu(\Set{x}{f(x) \ge n})\biggl]$$
	\item Остался случай произвольной функции $f(x)$. Разобьем ее на две: одна функция совпадает с $f(x)$ там, где та положительна, и равняется 0 в остальных случаях,
	другая равна $|f(x)|$ там, где $f(x)$ отрицательна, и 0 иначе. 
	$$ f^+(x) = \max\{0, f(x)\},\quad f^-(x) = -\min\{0, f(x)\}$$
	Для каждой из этих функций интеграл определен по предыдущему пункту. Пользуясь тем, что $f(x) = f^+(x) - f^-(x)$, определим интеграл так:
	$$  \int\limits_0^1 f(x) \mu(dx) =  \int\limits_0^1 f^+(x) \mu(dx) - \int\limits_0^1 f^-(x) \mu(dx) $$
	В случае, если оба интеграла в правой части расходятся, интеграл от $f(x)$ не определен. 
	Так как $|f(x)| = f^+(x) + f^-(x)$, сходимость интеграла Лебега от модуля функции (абсолютная сходимость) эквивалента сходимости интеграла от самой функции
	(то есть обычной сходимости). Это следует из того, что интеграл сходится только в случае конечности обоих интегралов правой части (иначе он не определен), откуда 
	следует конечность их суммы и разности. Таким образом, для интеграла Лебега не существует условно сходящихся функций.
\end{itemize}

	Аналогично вводим интеграл на каждом полуинтервале $[i, i+1), \quad i \in \mathbb{Z}$. Тогда на всей прямой интеграл по множеству $A \subset \Real$ будет определяться так:
	$$ \int\limits_A f(x) \mu(dx) = \sum\limits_{i=-\infty}^{+\infty} \int\limits_{A \bigcap [i, i+1)} f(x) \mu(dx) $$
\begin{Ex}
С помощью интеграла Лебега можно считать интегралы от функций, об интегрировании которых раньше было страшно даже подумать. Например, от функции Дирихле:
  \[
  	D_{[0,1)}(x) = 
  	\begin{cases}
  		1, x \in [0,1) \setminus \mathbb{Q} \\
  		0, x \in \mathbb{Q}
  	\end{cases}
  \]
  Данная функция является финитной, а именно $D_{[0,1)}(x) = \Ind([0,1) \setminus \mathbb{Q})$. Поэтому по первому пунккту
  $$ \int\limits_0^1 D_{[0,1)}(x) \mu(dx) = 1 $$
\end{Ex}

  В дальнейшем $\mu(dx)$ будет опускаться обозначаться просто как $dx$ или $dy$ чтобы подчеркнуть, что считается именно интеграл Лебега.
  
 \newpage
\section{Виды распределений}

Распределения случайных величин можно разделить на 3 типа: непрерывные, дискретные и сингулярные.

\begin{Def}
	Случайная величина $\xi$ называется \underline{абсолютно непрерывной}, если существует интегрируемая функция $p_\xi(x) \ge 0, \ x \in \Real$ такая, что
	функция распределения $\xi$ является почти всюду (за исключением не более, чем счетного числа точек) дифференцируемой функцией и представима в виде
	$$F_\xi(x) = \int\limits_{-\infty}^x p_\xi(y)dy$$
	Отсюда следует, что функция распределения непрерывна на $\Real$. $p_\xi(x)$ называется \underline{плотностью распределения},
	и почти всюду выполнено $p_\xi(x)=F_\xi'(x)$.
	Плотность, вообще говоря, определена не однозначно.
\end{Def}

\begin{Def}
	Случайная величина $\xi$ называется \underline{дискретной}, если множество точек роста не более, чем счетно, но распределение не является сингулярным, или, 
	другими словами $\exists B = \{x_1, x_2, \ldots\} \colon \Pro(\xi \in B) = 1$.
\end{Def}

\begin{Def}
	Случайная величина $\xi$  называется \underline{сингулярной}, если $F_\xi$ непрерына, и $\exists B \in \Bor \colon \mu(B) = 0, \ \Pro(\xi \in B) = 1$, то есть множество значений случайной величины имеет меру 0, но вероятность попасть в каждую точку этого множества так же нулевая.
\end{Def}

Пара слов о жизненном смысле определений: непрерывная случайная величина имеет областью значений континуальное множество, при этом вероятность попасть в отдельно взятую точку нулевая. Пример: равномерное распределение по отрезку. Плотность же отражает вероятность попасть в ту или иную область: интеграл по области равен этой вероятности. Дискретная случайная величина принимает конечное или счетное множество значений, вследствие этого имеет ступенчатую функцию распределения, например, бросок монетки имеет дискретное распределение. Сингулярное распределение --- это крокодил, который 
не встречается в жизни и будет рассмотрен отдельно.

\begin{St}
	Дискретная случайная величина имеет не более, чем счетное число скачков.
\end{St}
\begin{Proof}
Из свойств функции распределения следует, что дискретная величина имеет не больше двух скачков величины больше $\frac12$. Аналогично, скачков величины больше $\frac13$ не больше 3. То есть скачков величины больше $\frac1n$ не более n. Для любого скачка можно указать $n \in \mathbb{N}$ такое, что величина, этого скачка больше 
$\frac1n$. Значит,каждому скачку можно поставить в соответствие $n$, множество которых счетно. При этом для каждого $n$ существует не более чем счетное число скачков, ему соответсвующих (величины $>\frac1n$). А так как объединение не более, чем счетного числа не более, чем счетных множеств, не более, чем счетно, получаем 
требуемое. 
\end{Proof}\\ \\
\begin{Ex}
Для полного счастья приведем пример сингулярной случайной величины. Пусть функция распределения - так называемая лестница Кантора (см. рисунок).
\parbox[b][3 cm][t]{20mm}{\includegraphics[height=30mm]{kantor}}
\hfill
\parbox[b][3 cm][t]{100mm}{
	Посчитаем меру множества, на котором функция константа, то есть точки этого множества не будут точками роста: сначала это одна ступенька длины 1/3, потом две длины 1/9, и т.д.

}\\
	$$ \frac13 + \frac29 + \frac4{27} = \frac13 \sum\limits_{k=1}^\infty(\frac23)^{k-1} = 1$$
	Тогда множество точек роста имеет меру 0 в силу свойства аддитивности меры.
\end{Ex}
\\

Вообще говоря, существуют менее изысканные примеры сингулярных распределений. Например, при стрельбе из лука в круглую мишень распределение будет сингулярным,
если стрелок попадает только в точки одной прямой. В самом деле, двумерная мера прямой равна 0, как и вероятность попасть в каждую отдельную точку. 

\begin{Th} [Лебега]
	Любую случайную величину можно представить в виде суммы дискретной, абсолютно непрерывной и сингулярной случайной величины. То есть 
	$$ F(x) = \alpha_dF_d(x) + \alpha_cF_c(x) + \alpha_sF_s(x), \quad \alpha_d + \alpha_c + \alpha_s = 1$$
\end{Th}
\begin{Proof}
вышло и не вернулось
\end{Proof}

\newpage
\section{Характеристики случайных величин}

\underline{Математическое ожидание} (обозначается $\Expec$) обобщает понятие среднего арфиметического для произвольной случайной величины и показывает, какие значения в среднем принимает случайная величина. Оно, как и интеграл Лебега, вводится в несколько этапов. В этом определении вероятность $\Pro$ играет роль
Лебеговой меры $\mu$.

\begin{itemize}
 	\item Если $\xi(\omega) = \sum\limits_{i=1}^kx_i \Ind(A_i) \quad (i \not= j \Rightarrow A_iA_j = \varnothing, \ \bigcup_{i=1}^k A_i= \Omega)$. 
 	$$\Expec \xi = \int\limits_{\Omega} \xi(\omega) \Pro(d\omega) \equiv \sum\limits_{i=1}^k x_i \Pro(\Set{\omega}{\xi(\omega)=x_i}) =  \sum\limits_{i=1}^k x_i \Pro(A_i)$$
	\item $\xi(\omega) \ge 0$. В этом случае аналогично интегралу Лебега
	$$ \Expec \xi = \int\limits_{\Omega} \xi d\Pro = \lim_{n \to \infty}  \biggl[  \sum\limits_{k=1}^{n2^n} \frac{k-1}{2^n} \Pro\biggl(\Set{\omega}{\frac{k-1}{2^n} \le \xi(\omega) < \frac{k}{2^n}}\biggl) 
	+ n \Pro(\Set{\omega}{\xi(\omega) \ge n})\biggl]$$
	\item Для произвольной $\xi(\omega)$ вводятся
	$$ \xi^+(\omega) = \max\{0, \xi(\omega)\},\quad \xi^-(\omega) = -\min\{0, \xi(\omega)\}$$
	$$ \Expec \xi = \Expec \xi^+ - \Expec \xi^-$$
\end{itemize}

Вспомним, что любая случайная величина $\xi$ индуцирует вероятностное пространство $(\Real, \Bor_\xi, \Pro_\xi)$. Тогда, поскольку $\Pro_\xi(dx)$ есть вероятность попасть
в $dx$, выразим ее через функцию распределения $\Pro_\xi(dx) = F_\xi(x + dx) - F_\xi(x) = dF_\xi(x)$ Тогда перепишем матожидание в более привычной форме
$$\Expec\xi = \int\limits_{\Omega} \xi(\omega) \Pro(d\omega) = \int\limits_{-\infty}^{+\infty} x \Pro_\xi(dx) = \int\limits_{-\infty}^{+\infty} x dF_\xi(x)$$

Перечислим некоторые свойства математического ожидания:
\begin{enumerate}
	\item $\Expec(\xi + a) = \Expec\xi + a \quad \forall a \in \Real$
	\item $\Expec(a\xi) = a\Expec\xi \quad \forall a \in \Real$
	\item $\Expec(\xi + \eta) = \Expec\xi + \Expec\eta$ (здесь подразумевается, что существуют два из трех математических ожидания, из чего следует существование третьего)
\end{enumerate}
\begin{Ex}
Рассмотрим дискретную случайную величину $\xi$, которая принимает значения $n$ с вероятностью $\frac{c}{n^2} \ (c = \frac{6}{\pi^2})$. По определению
$$ \Expec\xi = \sum\limits_{n=1}^{\infty} n \frac{c}{n^2} $$
Данный ряд, очевидно, расходится. Сиутацию не спасет даже рассмотрение случайной величины $\eta$, принимающей значения $\pm n$ с вероятностью $\frac{c}{2n^2}$, 
которая имеет среднее значение 0, 
поскольку $\Expec\eta = \sum\limits_{n=1}^{\infty} \frac{c}{2n} - \sum\limits_{n=1}^{\infty} \frac{c}{2n}$, что не определено, поскольку интегралы Лебега от $\eta^+$ и $\eta^-$ расходятся.\\
\end{Ex}
\begin{Ex}
Другим примером является распределение Коши с плотностью $p_\xi(x) = \frac{1}{\pi(1+x^2)}$. График этой функции симметричен относительно 0 и похож на горку, из чего
методом пристального взгляда можно сделать вывод, что средним значением должно быть 0. Однако  $
\int\limits_{-\infty}^{+\infty} x dF_\xi(x) =  \int\limits_{-\infty}^{+\infty} x p_\xi(x)dx$ расходится, поэтому математического ожидания не существует.
\end{Ex}

\begin{Def}
\underline{Моментом порядка $k$} случайной величины $\xi$ называется $\Expec\xi^k$ \\
\underline{Абсолютным моментом порядка $k$} случайной величины $\xi$ называется $\Expec|\xi|^k$ \\
\underline{Центральным моментом порядка $k$} случайной величины $\xi$ называется $\Expec(\xi - \Expec\xi)^k$ \\
\end{Def}

\begin{Def}
\underline{Квантилью} случайной величины $\xi$ порядка $q$ называется величина $l_\xi(q)$:
\[
   	l_\xi(q) \colon 
  	\begin{cases}
  		\Pro(\xi \le l_\xi(q)) \ge q \\
  		\Pro(\xi \ge l_\xi(q) \ge 1-q
  	\end{cases}
  \]
  
  В случае $q=\frac12$ квантиль называется \underline{медианой} и обозначается $\Med\xi$.
  
  Если $q=\frac14$, то $l_\xi$ называется \underline{квартилью}, если $q=\frac1{10}$, \underline{децилью}, а если $q=\frac{1}{100}$ --- \underline{перцентилью}.
\end{Def}

Жизненный смысл медианы заключается в том, что она является точкой, вероятность попасть левее которой равна пероятности попасть правее нее. Аналогично можно 
сказать про квантиль любого порядка. Квантиль определена не единственным образом: пусть $\xi$ принимает значения $\{0, 1\}$ с вероятностями $\frac12$. Тогда медианой
$\xi$ может быть любая точка из отрезка $[0,1]$.

<<<<<<< HEAD
С матожиданием и мединой связана задача о <<деловых людях>>. (здесь будет ссылка) 
\newpage

=======
\begin{Def}
\underline{Интерквантильный размах} --- величина $R_\xi = l_\xi(\frac34) - l_\xi(\frac14)$ --- длина отрезка, вероятность попасть в который равна $\frac12$. 
\end{Def}

С матожиданием и медианой связана задача о <<деловых людях>>. (здесь будет ссылка) 

\begin{Def}
\underline{Мода} случайной величины $\xi$ --- это наиболее вероятное значение случайной величины. Оюозначается $\Mod\xi$.
\end{Def}

При наблюдении случайной величины важно знать не только её среднее значение (матожидание), но и то, как сильно она от него отклоняется (например, измерение линейнкой в среднем дает правильный результат, однаком необходимо знать погрешность измерения). В связи с этим вводится понятие дисперсии.

\begin{Def}
Пусть для случайной величины $\xi$ существуют конечный $\Expec\xi$ и $\Expec\xi^2$.  \underline{Дисперсией} назывется величина, равная 
$$ \Disp\xi = \Expec(\xi - \Expec\xi)^2$$
\end{Def}

Эту формулу можно привести к более простому для вычисления виду:
$$\Disp\xi = \Expec(\xi^2 - 2\xi\Expec\xi + (\Expec\xi)^2) = \Expec\xi^2 - 2\Expec\xi\Expec\xi - (\Expec\xi)^2 = \Expec\xi^2 - (\Expec\xi)^2$$


Перечислим некоторые свойства дисперсии. $\xi,\eta$ - случайные величины, $c \in \Real$.

\begin{enumerate}
	\item $\Disp\xi \ge 0$ как матожидание от неотрицательной функции
	\item $\Disp c\xi = c^2 \Disp\xi$ --- следует из определения и линейности матожидания.
	\item $\Disp(\xi+c) = \Expec(\xi + c - \Expec(\xi + c))^2 = \Expec(\xi - \Expec\xi)^2 = \Disp\xi$
	\item $\Pro(\xi = c) = 1 \Leftrightarrow \Disp\xi = 0$ --- отклонение равно нулю для константы
	\item $\Disp(\xi + \eta) = \Expec(\xi+\eta)^2 - (\Expec(\xi+\eta))^2 = \Expec(\xi^2+2\xi\eta + \eta^2) - (\Expec\xi)^2- 2\Expec\xi\Expec\eta - (\Expec\eta)^2 = \Disp\xi + \Disp\eta + 2(\Expec\xi\eta - \Expec\xi\Expec\eta)$
\end{enumerate}

\begin{Def}
\underline{Ковариация} двух случайных величин --- это величина, равная
$$ \Cov(\xi,\eta) = \Expec(\xi - \Expec\xi)(\eta - \Expec\eta)$$
\end{Def}

Ковариация положительна, если случайные величины одновременно отклоняются в одну сторону и отрицательная, если в разные. Формулу ковариации так же можно упростить, раскрыв скобки в определении:
$$\Cov(\xi,\eta) = \Expec\xi\eta - \Expec\xi\Expec\eta$$

Таким образом пятое свойство дисперсии можно переписать так:
\begin{itemize}
	\item[5.] $\Disp(\xi \pm \eta) = \Disp\xi + \Disp\eta \pm 2\Cov(\xi,\eta)$
\end{itemize}

\begin{Def}
\underline{Среднеквадратическое отклонение} --- величина $\sigma = \sqrt{\Disp\xi}$.
\end{Def}



\newpage
\section{Независимость случайных величин}
\begin{Def}
	Случайные величины $\xi, \eta$ называются \underline{независимыми}, если 
	 $$ \forall B_1, B_2 \in \Bor \quad \Pro(\xi \in B_1, \eta \in B_2) = \Pro(\xi \in B_1)\Pro(\eta \in B_2) $$
\end{Def}

\begin{St}
Для независимых случайных величин $\xi, \eta$ выполнено
$$\Expec\xi\eta = \Expec\xi\Expec\eta$$
\end{St}
\begin{Proof}
проведем только для случая дискреьных случайных величин.\\
Пусть 
\[
   	\xi = 
  	\begin{cases}
  		x_1, x_2, \ldots \\
  		p_1, p_2, \ldots
  	\end{cases}
  \]
  То есть $\xi$ приминмает значение $x_i$ с вероятностью $p_i$.
  \[
   	\eta = 
  	\begin{cases}
  		y_1, y_2, \ldots \\
  		q_1, q_2, \ldots
  	\end{cases}
  \]
  $$\Expec\xi\eta=\sum\limits_{i,j} x_iy_j\Pro(\xi=x_i, \eta=y_j) = \sum\limits_{i,j} x_iy_j\Pro(\xi=x_i)\Pro(\eta=y_j) =  \sum\limits_{i} x_i\Pro(\xi=x_i)\sum\limits_j y_j\Pro(\eta=y_j) = \Expec\xi\Expec\eta$$
\end{Proof}

Для независимых случайных величин
$$\Cov(\xi,\eta) = \Expec\xi\eta - \Expec\xi\Expec\eta = 0$$

Обратное, вообще говоря, неверно: пусть мы равновероятно выбираем одну из точек $(-1, 0), (0, 1), (1, 0), (0, -1)$. Каждая координата принимает значения $-1, 0, 1$, но координаты зависимы, так как $\Pro(x=0,  y=0) = 0$ (никогда не выбираем (0,0)), а $\Pro(x=0)\Pro(y=0) = \frac12\frac12 \not=0$. Однако
$$\Expec x = \Expec y = \Expec xy = 0$$ в силу симметрии задачи. Отсюда $\Cov(x,y) = 0$.

Таким образом ковариация показывает зависимость величин, однако не дает представления, насколько они зависимы. Для это вводится понятие коэффициента корреляции ---- нормированная ковариация.

\begin{Def}
\underline{Коэффициент корреляции} --- величина, описываемая формулой
$$\rho(\xi, \eta) = \frac{\Cov(\xi, \eta)}{\sqrt{\Disp\xi\Disp\eta}}$$
\end{Def}

Коэффициент корреляции является псевдоскалярным произведением, то есть выполнены все аксиомы скалярного произведения, кроме половины четвертой. Поэтому для нее выполнено неравенство Коши-Буняковского.

Некоторые свойства коэффициента корреляции:
\begin{enumerate}
	\item $|\rho(\xi, \eta)| \le 1$ --- неравенство Коши-Буняковского
	\item $|\rho(\xi, \eta)| = 1 \Leftrightarrow \exists a, b \colon \quad \Pro(\xi = a\eta + b) = 1$ --- равенство достигается, если случайные величины линейно зависимы
	\item для независимых случайных величин $\rho(\xi, \eta) = 0$
\end{enumerate} 



\newpage
\section{Вероятностные неравенства}
\begin{Lem}
Для любой неотрицательной неубывающей функции $g(x)$ выполнено неравенство 
$$\Pro(|\xi| > x) \le \frac{\Expec g(|\xi|)}{g(x)}$$
\end{Lem}
\begin{Proof} \\
$$\Expec g(|\xi|) = \Expec g(|\xi|) \Ind(|\xi| \ge x) + \Expec g(|\xi|) \Ind(|\xi| < x) \ge \Expec g(|\xi|) \Ind(|\xi| \ge x) \ge g(x)\Expec\Ind(|\xi| \ge x) = g(x)\Pro(|\xi| > x)$$
Здесь мы воспользовались представлением $1 = \Ind(A) + \Ind(\overline{A})$, затем неотрицательностью функции и, следовательно, ее матожидания. Далее использовалась монотонность функции, и в последнем переходе тождество $\Expec \Ind(A) = 1 \Pro(A) + 0 \Pro(\overline{A}) = \Pro(A)$
\end{Proof}

Из этой леммы слкдуют два полезных неравенства.

\begin{Th} [неравенство Маркова]
Для любой случайной величины $\xi$, имееющей конечное $\Expec|\xi|$, выполнено
$$\Pro(|\xi| > \epsilon) \le \frac{\Expec|\xi|}{\epsilon}$$
\end{Th}
\begin{Proof}\\
В неравенстве леммы возьмем $g(x) = x$.
\end{Proof}

\begin{Th} [неравенство Чебышева]
Для любой случайной величины $\xi$, имееющей конечный первый и второй момент, выполнено
$$\Pro(|\xi - \Expec\xi| > \epsilon) \le \frac{\Disp\xi}{\epsilon^2}$$
\end{Th}
\begin{Proof}\\
В неравенстве леммы возьмем $g(x) = x^2$.
\end{Proof}

\newpage
\section{Испытания Бернулли}

\begin{Def}
Дискретная случайная величина $\xi$ имеет \underline{распределение Бернулли}, если
$$ \Pro(\xi = x_1) = p,\ \Pro(\xi = x_2) = q = 1 - p, \quad x_1 \not= x_2$$
\end{Def}

\begin{Def}
\underline{Схема Бернулли} --- последовательность испытаний, удовлетворяющих следующим условиям:
\begin{enumerate}
	\item Дихотомичность --- у каждого испытания два исхода, называемые <<успехом>> и <<неудачей>> или, сокращенно У/Н.
	\item Независимость --- результаты испытаний являются независимыми событиями.
	\item Однородность --- вероятности успеха в каждом испытании равны.
\end{enumerate}
\end{Def}

Из определения следует, что одно испытание имеет распределение Бернулли. Элементарным исходом в схеме Бернулли из $n$ испытаний будет являться
$$\omega = (x_1, x_2, \ldots, x_n),$$
где $x_i$ --- результат испытания $i$. Пусть в элементарном исходе $k$ успехов. Тогда
$$\Pro(\omega) = p^kq^{n-k}$$

Покажем, что веорятность, введенная таким образом, удовлетворяет всем аксиомам вероятностной меры. Неочевидной здесь является только проверка нормировки, то есть надо доказать, что
$$ \sum\limits_{\omega}\Pro(\omega) = 1$$
Для этого просуммируем все исходы по числу успехов (обозначим $\mu_n$).
$$ \sum\limits_{\omega}\Pro(\omega) = \sum\limits_{k = 0}^n\sum\limits_{\omega\colon\mu_n=k}\Pro(\omega) = \sum\limits_{k = 0}^n\sum\limits_{\omega\colon\mu_n=k}p^kq^{n-k} = 
\sum\limits_{k = 0}^nC_n^kp^kq^{n-k} = (p+q)^n = 1$$

Рассмотрим теперь некоторые важные распределения, связанные со схемой Бернулли.

Уже рассмотренная величина $\mu_n$, равная числу успехов в $n$ испытаниях Бернулли, является случайной величиной с распределением
$$\forall k = 0, 1, \ldots, n \quad \Pro(\mu_n=k) = C_n^kp^kq^{n-k}$$
Это следует из того, что исходов с $k$ успехами ровно $C_n^k$, а вероятность каждого равна $p^kq^{n-k}$.
Такое распределение называется \underline{биномиальным} и обозначается $Bi(n,p)$.

Рассимотрим случайные величины $X_i = \Ind(A_i)б \quad i = 1, \ldots, n$, где $A_i$ --- успех в $i$-м испытании. Каждая такая величина имеет распределение Бернулли.
Тогда число успехов можно представить так:
$$\mu_n = \sum\limits_{i=1}^nX_i$$

Найдем матожидание и дисперсию $\mu_n$.
$$\Expec\mu_n = \Expec\sum\limits_{i=1}^nX_i = \sum\limits_{i=1}^n\Expec X_i = \sum\limits_{i=1}^n(1p + 0 q) = np$$
$$\Expec X_i^2 = 1p+0q = p$$
$$\Disp X_i = \Expec X_i^2 - (\Expec X_i)^2 = p - p^2 = pq$$
В силу независимости испытаний дисперсия линейна относительно сложения
$$\Disp \mu_n = \Disp\sum\limits_{i=1}^n  X_i  =\sum\limits_{i=1}^n  \Disp X_i = \sum\limits_{i=1}^n pq = npq$$

\begin{Th} [Бернулли]
Для случайной величины с распределением $Bi(n,p)$
$$\Pro(|\frac{\mu_n}{n} - p| \ge \epsilon) \le \frac{npq}{n^2\epsilon^2}$$
\end{Th}
\begin{Proof} \\
Домножим обе части неравенства на $n$ и воспользуемся неравенством Чебышева:
$$\Pro(|\mu_n - pn| \ge n\epsilon) \le \frac{\Disp(\mu_n)}{(n\epsilon)^2} = \frac{npq}{n^2\epsilon^2}$$
\end{Proof}

Пусть $f(x) \in C[0, 1]$.
\begin{Def}
\underline{Многочленом Бернштейна} называется функция 
$$ B_n(x, f) = \Sum{k}{0}{n}C_n^kx^k(1-x)^{n-k}f(\frac{k}{n}), \quad x \in [0, 1]$$
\end{Def}
Заметим, что $B_n(x, f) = \Expec f(\frac{\mu_n}n),\quad  \mu_n \sim Bi(n, x)$. (запись $\xi \sim$ *destrname*  означает, что случайная величина $\xi$ имеет распределение *destrname*).

\begin{St}
$$B_n(x, f) \rightrightarrows f(x), \quad x \in [0,1]$$
\end{St}
\begin{Proof} \\
Пользуясь тем, что $\sum\limits_{k = 0}^nC_n^kx^k(1-x)^{n-k} = 1$ получим
$$|B_n(x, f) - f(x)| = |\sum\limits_{k = 0}^nC_n^kx^k(1-x)^{n-k}f(\frac{k}n) - \sum\limits_{k = 0}^nC_n^kx^k(1-x)^{n-k}f(x)| \le 
\sum\limits_{k = 0}^nC_n^kx^k(1-x)^{n-k}|f(\frac{k}n) - f(x)|$$
Разобьем данную сумму на две:
$$\Sum{k}{0}{n} = \sum\limits_{|\frac{k}n - x| < \delta} + \sum\limits_{|\frac{k}n - x| \ge \delta}$$
Выберем $\delta$ так, чтобы первая сумма была меньше $\frac{\epsilon}2$. Это всегда можно сделать, так как функция $f(x)$ непрерывна, а 
$\sum\limits_{|\frac{k}n - x| < \delta}C_n^kx^k(1-x)^{n-k} \le 1$.
Во второй сумме ограничим модуль числом $M = 2\sup\limits_{[0,1]} f(x)$ (1-я теорема Вейерштрасса), а к оставшейся сумме применим неравенство Чебышева, поскольку она равна вероятности 
$\Pro(|\mu_n - nx| \ge n\delta)$. 
$$\sum\limits_{|\frac{k}n - x| \ge \delta}\ldots \le 2M\sum\limits_{|\frac{k}n - x| \ge \delta}C_n^kx^k(1-x)^{n-k} \le 2M\frac1{n\delta^2}$$
Выбором $n$ сделаем вторую сумму меньше $\frac{\epsilon}2$, доказав, тем самым, равномерную сходимость.
\end{Proof}

Пусть в схеме Бернулли с вероятностью успеха $0 < p \le 1$ величина $\eta$ равна номеру первого успеха. $\eta$ является случайной величиной, принимающей натуральные значения. Найдем распределение $\eta$: серия, в которой первый успех появляется в $k$-м испытании выглядит так: НН...НУ. Отсюда
$$\forall k \in \Nat \quad \Pro(\eta = k) = (1-p)^{k-1}p = q^{k-1}p $$
Так как $\Omega = \Set{\omega_k = \underbrace{0\ldots0}_k1}{k \in \Nat}$, то 
$$\Sum{k}{1}{n}\Pro(\omega_k) = \Sum{k}{1}{n} p(1-p)^{k-1} = 1$$
Такое распределение называется \underline{геометрическим с параметром $p$}.

Пусть $\xi$ имеет геометрическое распределение с параметром $p$. Тогда
$$\Expec\xi = \Sum{k}{1}{\infty}kp(1-p)^{k-1} = -p\Sum{k}{1}{\infty}\frac{d}{dp}(1-p)^k = -p \frac{d}{dp}(\frac{1-p}{p}) = \frac1{p}$$
Аналогично, дифференцируя степенные ряды, получим диспресию
$$\Disp\xi = \frac{q}{p^2}$$

Пусть теперь $\theta$ --- число неудач до $r$-го успеха. Тогда
$$\forall k \in\Int_0 \quad \Pro(\theta = k) = p^rq^kC_{k+r-1}^{k}$$
Это следует из того, что всего испытаний было $r+k$, на последнем месте успех, а до него как-то располагаются $k$ неудач и $r-1$ успех.
Такое распределение называется \underline{отрицательным биномиальным} с параметрами $r, p$.

\newpage
\section{Предельные теоремы}

\begin{Th} [Пуассон]
Пусть $\lambda = np$. Тогда при малых $p$ и больших $n$ можно использовать приближение
$$\Pro(\mu_n = k) = C_n^kp^kq^{n-k} \approx \frac{e^{-\lambda}\lambda^k}{k!}$$
\end{Th}
\begin{Proof}
Докажем индукцией по $k$. При $k = 0$
$$\Pro(\mu_n = 0) = (1-p)^n = (1 - \frac{\lambda}{n})^n \longrightarrow e^{-\lambda} \quad (n \to \infty)$$
$$\frac{\Pro(\mu_n = k)}{\Pro(\mu_n = k-1)} = \frac{n!}{k!(n-k)!}p^kq^{n-k} \frac{(k-1)!(n-k+1)!}{n!} \frac{1}{p^{k-1}q^{n-k+1}} = \frac{(n-k+1)p}{kq}$$
$$\Pro(\mu_n = k) = \frac{(n-k+1)p}{kq}\Pro(\mu_n = k-1) = \frac{(n-k+1)\frac{\lambda}n}{k(1-\frac{\lambda}n)}\Pro(\mu_n = k-1)$$

Воспользуемся предположением индукции для $k-1$.
$$\Pro(\mu_n = k) \to \frac{(n-k+1)\frac{\lambda}n}{k(1-\frac{\lambda}n)} \frac{e^{-\lambda}\lambda^{k-1}}{(k-1)!} \longrightarrow \frac{e^{-\lambda}\lambda^{k}}{k!} (n \to \infty)$$
\end{Proof}

Следующая оценка формализует <<малость>> $p$ и <<величину>> $n$:
$$\sup\limits_k |\Pro(\mu_n = k) - \frac{e^{-\lambda}\lambda^k}{k!}| \le 2np^2$$
Таким образом, зная $n, p$ всегда можно оценить сверху погрешность аппроксимации.\\
\begin{Why}
При очень большом числе испытаний ни один нормальный компьютер не способен вычислить  $ C_n^kp^kq^{n-k}$, а уж тем более сложить их. Данная теорема позволяет сводить вычисление таких сложных вещей к вычислению экспонент.
\end{Why}

Распределение величины $\xi$ такое, что $\forall k \in \Int_+\quad  \Pro(\xi = k) =\frac{e^{-\lambda}\lambda^k}{k!}$, называется \underline{распределением Пуассона} 
и обозначается $\Pois(\lambda)$

Одно из приложений распределения Пуассона --- это пуассоновские потоки. Пусть во времени происходят некоторые события, которые мы фиксируем. $\lambda$ --- интенсивность потока --- показывает среднее число событий за единицу времени. Тогда число произошедших событий на отрезке $[0, t]$ равно $\xi_t\colon \Pro(\xi_t = k) = \frac{e^{-\lambda t}(\lambda t)^k}{k!}$.

Свойства пуассоновского распределения. ($\xi \sim \Pois(\lambda)$)
\begin{enumerate}
	\item $\Expec\xi = \lambda$
	\item $\Disp\xi = \lambda$
	\item $\xi_i \sim \Pois(\lambda_i) \quad i = 1, \ldots, n \Rightarrow \Sum{i}{1}{n}\xi_i \sim \Pois(\Sum{i}{1}{n}\lambda_i)$
\end{enumerate}

\begin{St}
Пусть независимые случайные величины $\xi_i \sim \Pois(\lambda_i), \quad i = 1, 2$. Тогда условное распределение $\xi_1$ при условии $\xi_1 + \xi_2 = n$ имеет биномиальное распределение с параметрами $n, \frac{\lambda_1}{\lambda_1 + \lambda_2}$, то есть
$$\Pro(\xi_1 = k | \xi_1 + \xi_2 = n) = C_n^kp^k(1-p)^{n-k}, \quad p = \frac{\lambda_1}{\lambda_1 + \lambda_2}$$
\end{St}
\begin{Proof}
$$\Pro(\xi_1 = k | \xi_1 + \xi_2 = n) = \frac{\Pro(\xi_1 = k, \xi_1 + \xi_2 = n}{\Pro(\xi_1 + \xi+2 = n)} = \frac{\Pro(\xi_1 = k, \xi_2 = n - k}{\Pro(\xi_1 + \xi+2 = n)}=$$
В числителе воспользуемся независимостью $\xi_1, \xi_2$, а в знаменателе свойством 3 пуассоновского распределения:
$$=\frac{e^{-\lambda_1}\frac{\lambda_1^k}{k!}e^{-\lambda_2}\frac{\lambda_2^{n-k}}{(n-k)!}}{e^{-(\lambda_1+\lambda_2)}\frac{(\lambda_1+\lambda_2)^n}{n!}} = 
C_n^k(\frac{\lambda_1}{\lambda_1 + \lambda_2})^k(\frac{\lambda_2}{\lambda_1 + \lambda_2})^{n-k}$$
\end{Proof}

\begin{Th} [Муавра-Лапласа]
Пусть $\sqrt{npq}$ велико. Тогда
$$\Pro(\mu_n = k) = \Pro(\frac{\mu_n - np}{\sqrt{npq}} = \frac{k - np}{\sqrt{npq}}) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}2} + o(1), \quad x \equiv \frac{k - np}{\sqrt{npq}}$$
\end{Th}
\begin{Proof} не было и не должно быть.
\end{Proof}

Рассмотрим функции
$$\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}2}$$
$$\Phi(x) = \int\limits_{-\infty}^x \frac{1}{\sqrt{2\pi}}e^{-\frac{u^2}2} du = \int\limits_{-\infty}^{x}\phi(u)du$$

Заметим, что 
$$\int\limits_{-\infty}^{+\infty} \phi(u)du = 1$$ 
Это верно, так как данный интеграл сводится заменой $t = \frac{x}{\sqrt{2}}$ к известному интегралу Пуассона.

Так как $\phi(x)$ неотрицательна, и интеграл от нее по всей прямой равен 1, она является плотностью некоторого абсолютно непрерывного распределения, которое называется \underline{стандартным нормальным распределением} и обозначается $N(0, 1)$.

Пусть $\xi \sim N(0,1)$.
$$\Expec\xi^k = \int\limits_{-\infty}^{+\infty}x^k\phi(x)dx$$
$$e^{h\xi}=1 + h\xi + \frac{(h\xi)^2}{2!} + \ldots$$
$$\Expec e^{h\xi} = 1 + h\Expec\xi + \frac{h^2}{2!}\Expec\xi^2 + \ldots \equiv \psi(h)$$
Функция $\psi(h)$ называется \underline{производящей функцией моментов}.
$$\psi(h) = \int\limits_{-\infty}^{+\infty}e^{hx}\phi(x)dx = \int\limits_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{hx-\frac{x^2}2}dx = \frac{e^{\frac{h^2}2}}{\sqrt{2\pi}}\int\limits_{-\infty}^{+\infty}e^{-\frac{(x-h)^2}2}dx = e^{\frac{h^2}2}$$
$$e^{\frac{h^2}2} = 1 + \frac{h^2}2 + \frac1{2!}(\frac{h^2}2)^2 + \ldots$$
Приравнивая это равенство к предыдущему разложению получим, что все нечетные центральные моменты равны 0.
$$\Expec\xi^{2n-1} = 0$$
$$\frac{1}{n!2^n} = \frac{\Expec\xi^{2n}}{(2n)!} \Rightarrow \Expec\xi^{2n} = \frac{(2n)!}{n!2^n} = \frac{2n(2n-1)\ldots n \ldots 1}{n!2^n} = (2n-1)!!$$

Последняя формула позволяет очень быстро получить нужный момент, не считая интеграл по честям много раз.

\begin{Th}[Интегральная теорема Муавра-Лапласа]
Пусть $\sqrt{npq}$ велико. Тогда
$$\Pro(m_1 \le \mu_n \le m_2) = \Pro(\frac{m_1 - np}{\sqrt{npq}} \le \frac{\mu_n - np}{\sqrt{npq}} \le \frac{m_2 - np}{\sqrt{npq}}) \approx \int\limits_{x_1}^{x_2}\phi(x)dx, \quad x1 \equiv \frac{m_1 - np}{\sqrt{npq}}, x2 \equiv\frac{m_2 - np}{\sqrt{npq}}$$
\end{Th}

С данной теоремой связана <<Задача о докторе Споке>>. (Добавить ссылку)

Рассмотрим теперь случайную величину $\xi$ с геометрическим распределением. Поскольку $\Expec\xi = \frac1p$, 
$$p\xi=\frac{\xi}{\Expec\xi}$$

При стремлении вероятности успеха к нулю, номер первого успеха будет стремиться к бесконечности, как и матожидание $\xi$. Однако их отношение будет иметь конечный предел:
$$\forall x > 0 \quad \Pro(p\xi > x) = \Pro(\xi > \frac{x}p) = \Sum{k}{[\frac{x}{p}] + 1}{\infty}p(1-p)^{k-1} = \frac{p(1-p)^{[\frac{x}p]}}p = (1-p)^{[\frac{x}p]}$$ 
$$\lim\limits_{p \to 0} \Pro(p\xi > x) = \lim\limits_{p \to 0}(1-p)^{[\frac{x}p]} = e^{-x}$$
В последнем переходе от дробной части можно избавиться, так как она не вносит никакого вклада в предел.
Итак, получили, что
$$\lim\limits_{p \to 0} \Pro(p\xi < x) = 1 - e^{-x}$$

Это частный случай \underline{показательного} (экспоненциального) распределения. В общем случае оно выглядит так:
$$F(x) = 1 - e^{-\lambda x}$$
$$p(x) = \lambda e^{-\lambda x}$$
Матожидание случайной величины, распределенной показательно, имеет вид
$$\int\limits_0^{\infty} x\lambda e^{-\lambda x}dx = \frac1{\lambda}$$

Показательно распределение обладает интересным свойством: свойством отсутвия последействия (или отсутствия памяти). Предположим, что вы ждете автобус на 
остановке, а время между автобусами $\xi$ имеет показательное распределение. Тогда вероятность того, что вы прождете автобус еще $t$ никак не зависит от того, 
как долго ($\tau$) вы уже ждете. Формализуем это:
$$ \Pro(\xi > t + \tau | \xi > \tau) = \Pro(\xi > t)$$
По определению условной вероятности
$$\Pro(\xi > t + \tau | \xi > \tau) = \frac{\Pro(\xi > t + \tau, \xi > \tau)}{\Pro(\xi > \tau)} = \frac{\Pro(\xi > t + \tau)}{\Pro(\xi > \tau)} = \frac{e^{-\lambda(t+\tau)}}{e^{-\lambda \tau}} = e^{-\lambda t}$$

Показательное распределение является единственным обладающим таким свойством в классе абсолютно непрерывных распределений. В классе дискретных распределений таким свойством обладает только геометрическое распределение (введенное как число неудач до первого успеха).

\newpage
\section{Векторные случайные величины}
Векторную случайную величину можно определить двумя способами:
\begin{enumerate}
	\item Сказать, что $\xi = (\xi_1, \ldots, \xi_n)$ является векторной случайной величиной, если ее координаты являются случайными величинами.
	\item Дать определение аналогично определению одномерной случайной величины, то есть рассмотреть $(\Real^n, \Bor_n)$, где $\Bor_n$ --- $n$-мерная борелевская
	$\sigma$-алгебра, то есть минимальная $\sigma$-алгебра, содержащая все параллелепипеды (аналогично интервалам в одномерном случае). Тогда $\xi$ --- случайная величина, если $$\forall B \in \Bor_n \colon \xi^{-1}(B) \in \Ev$$. 
\end{enumerate}

\begin{Def}
\underline{Функция распределения} $$F_\xi(x_1, \ldots, x_n) = \Pro(\xi_1 < x_1, \ldots, \xi_n < x_n)$$
\end{Def}

Свойства функции распределения:
\begin{enumerate}
	\item Пусть $x, y \in \Real^n$ такие, что $x_i < y_i \quad i = 1, \ldots, n$. Тогда $F_\xi(x) \le F_\xi(y)$.
	\item $0 \le F_\xi(x) \le 1$
	\item $\lim\limits_{x_i \to -\infty} F_\xi(x) = 0$. То есть при стремлении одной координаты к $-\infty$ функция распределения стремится к 0, поскольку вероятность для соответсвующей координаты стремится к 0.
	\item Если же какую-то из координат устремить к бесконечности, то она не будет учитываться в вероятности, поэтому 
	$\lim\limits_{x_i \to +\infty} F_\xi(x) = F_{(\xi_1, \ldots, \xi_{i-1}, \xi_{i+1}, \ldots, \xi_n)}(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n)$.
	\item Непрерывна слева по каждому аргументу.
\end{enumerate}

\begin{Def}
\underline{Абсолютно непрерывная} случайная величина $\xi$ --- такая случайная величина, что
$$\forall B \in \Bor_n \quad \Pro(\xi \in \Bor_n) = \int\limits_B p_\xi(x_1, \ldots, x_n)dx_1\ldots dx_n$$ 
\end{Def}

$\xi_1, \ldots, \xi_n$ независимы в совокупности $\Leftrightarrow$ функцию распределения векторной случайной величины $\xi = (\xi_1, \ldots, \xi_n)$ можно представить произведением функций распределений координат, то есть 
$$F_\xi(x) = F_{\xi_1}(x_1)\ldots F_{\xi_n}(x_n)$$
В случае абсолютно непрерынвой векторной случайной величины это эквивалентно аналогичному выражению для плотностей:
$$p_\xi(x) = p_{\xi_1}(x_1)\ldots p_{\xi_n}(x_n)$$

\newpage
\section{Распределение функций от случайных величин}
Рассмотрим отображение $f\colon \Real^n \to \Real^n$ с якобианом $J_f = \frac{D(f_1(x), \ldots, f_n(x))}{D(x_1, \ldots, x_n)}$.
Если он отличен от 0, то существует обратная функция, и выполнено соотношение
$$J_fJ_{f^{-1}} = 1$$

\begin{Th}
Пусть  $f\colon \Real^n \to \Real^n$ --- достаточно гладкая функция с ненулевым якобианом. $\xi = (\xi_1, \ldots, \xi_n) \sim p_\xi(x)$. Рассмотрим случайную величину $\eta = f(\xi)$. Тогда
$$p_\eta(x) = p_\xi(f^{-1}(x))|J_{f^{-1}}(x)|$$
\end{Th}
\begin{Proof} \\
Вспомним формулу замены переменных в интеграле:
$$\int\limits_B \phi(x)dx = \int\limits_{f^{-1}(B)} \phi(f(y))|J_f(y)|dy$$
Тогда $\forall B \in \Bor_n$
$$\int\limits_B p_\xi(f^{-1}(x))|J_{f^{-1}(x)}|dx = \int\limits_{f^{-1}(B)} p_\xi(f(f^{-1}(B)))|J_{f^{-1}(x)}||J_{f(x)}|dx =$$
$$ \int\limits_{f^{-1}(B)}p_\xi(x)dx = \Pro(\xi \in f^{-1}(B)) =\Pro(f(\xi) \in B) = \Pro(\eta \in B)$$
\end{Proof}

Теперь найдем формулу для плотности суммы случайных величин.
Пусть $\xi = (\xi_1, \xi_2) \sim p_\xi(x)$
Рассмотрим $f\colon (x_1, x_2) \mapsto (x_1 + x_2, x_2)$. Тогда $$f^{-1} \colon (x_1, x_2) \mapsto (x_1 - x_2, x_2), \quad J_{f^{-1}}=1$$
Обозначим $\eta = f(\xi)$. Тогда по только что доказанной теореме 
$p_\eta(x_1, x_2) = p_{\xi}(x_1 - x_2, x_2)$
Тогда по свойству 4 функции распределения векторной случайной величины
$$p_{\xi_1 + \xi_2}(x) = \int\limits_{-\infty}^{+\infty}p_\xi(x-x_2, x_2)dx_2 = \text{\{независимость $\xi_1, \xi_2$\}} = \int\limits_{-\infty}^{+\infty}p_{\xi_1}(x-x_2)p_{\xi_2}(x_2)dx_2$$
В силу симметрии 
$$p_{\xi_1 + \xi_2}(x) = \int\limits_{-\infty}^{+\infty}p_{\xi_1}(x_1)p_{\xi_2}(x - x_2)dx_1$$

Эти формулы называются \underline{формулами свертки}. Существует аналогичная формула для функция распределения, но мы ее не доказываем, потому что это какой-то функан:
$$F_{\xi_1 + \xi_2}(x) = \int\limits_{-\infty}^{+\infty}F_{\xi_1}(x-y)dF_{\xi_2}(y) = \int\limits_{-\infty}^{+\infty}F_{\xi_2}(x-y)dF_{\xi_1}(y)$$

\newpage
>>>>>>> 7dd5d3f64923a3a7dffd71988cf1531076ed4dcd
\section{Виды сходимости случайных величин}

В этом разделе мы введем вагон непонятных определений, а потом постараемся запутаться еще больше, доказывая, какая сходимость круче, и ковыряясь в контрипримерах. Но для начала вспомним наших старых знакомых: измеримое пространство $(\Omega, \Ev)$, вероятностное пространство $(\Omega, \Ev, \Pro)$ и случайную величину $\xi \colon \Omega \rightarrow \Real$, которая обладает свойством
$$\forall B \in \Bor \quad \xi^{-1}(B) = \{ \omega \in \Omega \colon \xi(\omega) \in B \} \in \Ev$$

Пусть теперь $\xi, \xi_1, \xi_2, \dots$ "--- случайные величины на $(\Omega, \Ev)$.

\begin{Def}
Последовательность случайных величин $\{\xi_n\}$ \uline{сходится} к случайной величине $\xi$ \uline{почти наверное (с вероятностью 1)}, если вероятность множества тех элементарных событий, где она не сходится, равно нулю.
$$\{ \xi_n \} \xrightarrow{\text{п.н.}} \xi, \text{если} \quad \Pro(\Set{\omega}{\xi_n(\omega) \nrightarrow \xi}) = 0$$
\end{Def}

\begin{Ex}
Пусть $\xi_n$ принимает значение $n$ в рациональных точках числовой прямой и $0$ "--- в иррациональных. Тогда $\{\xi_n\} \xrightarrow{\text{п.н.}} 0$, так как мера множества рациональных чисел, где случайная величина расходится, "--- ноль.
\end{Ex}

\begin{Def}
Последовательность случайных величин $\{\xi_n\}$ \uline{сходится} к случайной величине $\xi$ \uline{в среднем порядка $r$}, если $r$-тый момент их разности сходится к нулю.
$$\{ \xi_n \} \xrightarrow{\text{(r)}} \xi, \text{если} \quad \Expec(\xi_n - \xi)^r \xrightarrow[n \rightarrow +\infty]{} 0$$
\end{Def}

\begin{Ex}
Возьмем в качестве множества $\Omega$ окружность длины 1, событиями будут борелевские множества, а вероятность введем как меру Лебега. События $A_n$ введем таким образом: $A_1$~"--- дуга длины $1/2$, отложенная от какой-то точки против часовой стрелки. $A_n$~"--- дуга длины $\frac{1}{n+1}$, отложенная против часовой стрелки от конца дуги $A_{n-1}$. Введем случайную величину: $\xi_n(\omega) = \Ind(\omega \in A_n)$
Покажем, что $\{\xi_n\}$ сходится к $0$ в среднем любого положительного порядка.
$$\Expec(\xi_n - \xi)^r = \Expec\xi_n^r = \Expec\left(\Ind(\omega \in A_n)\right)^r = \Pro(A_n) = 1/n \xrightarrow[n \rightarrow +\infty]{} 0$$
\end{Ex}

\begin{St}
Из сходимости в среднем, вообще говоря, не следует сходимость почти наверное.
\end{St}

\begin{Proof}
В рассмотренном выше примере $\xi_n(\omega)$ не сходится ни в одной точке окружности. Действительно, так как ряд $\sum_{n=1}^\infty \frac{1}{n}$ расходится, то для любой точки $\omega$ на окружности мы можем указать бесконечное число номеров $n_k$, таких что $\omega~\in~A_{n_k}$.~
\end{Proof}

\begin{St}
Из сходимости почти наверное, вообще говоря, не следует сходимость в среднем.
\end{St}

\begin{Proof}
Рассмотрим отрезок $[0, 1]$, с событиями, являющимися борелевскими множествами и вероятностью, введенной как мера Лебега. Положим $\xi_n(\omega) = e^{n} \Ind(\omega \in [0, 1/n])$. Тогда $\xi_n \xrightarrow{\text{п.н.}} 0$, но при этом $\forall r > 0 \quad \Expec\xi_n^r = e^{np}\Expec\Ind(\omega \in [0, 1/n]) = e^{np}/n$, а эта величина стремится к бесконечности, значит, сходимости в среднем нет.~
\end{Proof}

<<<<<<< HEAD
\begin{Why}
Чтобы было не так грустно, по ходу введения новых видов сходимости будем рисовать картинку, где все эти сходимости напиханы.
\end{Why}

\begin{tikzpicture}
	\node (prob1) at (0, 2) [rectangle,draw] {$\{ \xi_n \} \xrightarrow{\text{п.н.}} \xi$};
	\node (power) at (0, 0) [rectangle,draw] {$\{ \xi_n \} \xrightarrow{(r)} \xi$};
	\node (vertel) at (1, 1) [rectangle,draw] {Вертел я эти графики, 2 часа в ноль};
	\draw [double,thick,-{Implies[]}] (prob1) -> (power);
\end{tikzpicture}

=======
>>>>>>> 7dd5d3f64923a3a7dffd71988cf1531076ed4dcd
Введем еще один вид сходимости.

\begin{Def}
Последовательность случайных величин $\{\xi_n\}$ \uline{сходится} к случайной величине $\xi$ \uline{по вероятности}, если для любого сколь угодно малого положительного $\varepsilon$ вероятность таких событий, что модуль разности $\xi_n$ и $\xi$ больше $\varepsilon$, стремится к $0$.
$$\{\xi_n\} \xrightarrow{\Pro} \xi, \text{если} \quad \forall \varepsilon > 0 \quad \Pro(\Set{\omega}{|\xi_n(\omega) - \xi(\omega)| > \varepsilon}) \xrightarrow[n \rightarrow +\infty]{} 0$$
\end{Def}

\begin{Wtf}
На этом моменте может немного поплавиться мозг в попытках понять, чем сходимость по вероятности отличается от сходимости с вероятностью 1. Действительно, и там и там мы говорим, что вероятность тех событий, на которых случайная величина не сходится, равна нулю. Но разница в том, что в случае сходимости почти наверное мы сначала устремляем $n$ к бесконечности, а потом считаем вероятность, событий, когда не сходится, а в случае сходимости по вероятности мы сначала посчитали вероятность для какого-то фиксированного $n$, а потом устремились к бесконечности. 
\end{Wtf}

\begin{Ex}
Докажем, что та жуткая последовательность случайных величин на окружности сходится по вероятности к нулю. Действительно, 
$$\Pro(\Set{\omega}{|\xi_n(\omega) - 0| > \varepsilon}) = \Pro(\Set{\omega}{\xi_n(\omega) > \varepsilon}) = \Pro(\Set{\omega}{\xi_n(\omega) = 1}) = 1/n \xrightarrow[n \rightarrow +\infty]{} 0$$
\end{Ex}

\begin{St}
Только что рассмотренным примером мы доказали, что из сходимости по вероятности, вообще говоря, не следует сходимость почти наверное.
\end{St}

\begin{St}
Из сходимости по вероятности, вообще говоря, не следует сходимость в среднем.
\end{St}

\begin{Proof}
Для доказательства воспользуемся примером про отрезок. Докажем, что $\{\xi_n\} \xrightarrow{\Pro} 0$. Действительно, $\Pro{\Set{\omega}{\xi_n(\omega) > \varepsilon}} = 1/n \xrightarrow[n \rightarrow +\infty]{} 0$. Отсутствие сходимости в среднем мы уже доказали.
\end{Proof}

\begin{Th}
Из сходимости почти наверное следует сходимость по вероятности.
\end{Th}

\begin{Proof}
Сначала докажем, что $\{ \xi_n \} \xrightarrow{\text{п.н.}} \xi \quad \Leftrightarrow \quad \Pro\left(\sup \limits_{k \geqslant n} |\xi_k -\xi| \geqslant \varepsilon\right) \rightarrow 0$. Положим $A_n^\varepsilon = \Set{\omega}{|\xi_n -\xi| \geqslant \varepsilon}$, 
$A^\varepsilon = \varlimsup A_n^\varepsilon \equiv \bigcap\limits_{n=1}^\infty \bigcup\limits_{k \geqslant n} A_k^\varepsilon$. Тогда:
$$\Pro\left(\Set{\omega}{\xi_n(\omega) \nrightarrow \xi}\right) = 0 \quad \Leftrightarrow \quad \Pro\left(\bigcup_{\varepsilon > 0} A^\varepsilon\right) = 0 \quad \Leftrightarrow \quad \Pro\left(\bigcup_{m=1} A^{1/m}\right) = 0 \quad \Leftrightarrow$$
$$\Leftrightarrow \quad \Pro(A^{1/m}) = 0, m \geqslant 1 \quad \Leftrightarrow \quad \Pro(A^\varepsilon) = 0, \varepsilon > 0 \quad \Leftrightarrow \quad 
\Pro\left( \bigcup_{k \geqslant n} A_k^\varepsilon \right) \xrightarrow[n \rightarrow \infty]{} 0, \varepsilon > 0 \quad \Leftrightarrow$$
$$\Leftrightarrow \quad \Pro\left(\sup \limits_{k \geqslant n} |\xi_k -\xi| \geqslant \varepsilon\right) \xrightarrow[n \rightarrow \infty]{} 0 \quad \Rightarrow \quad \Pro\left( |\xi_k - \xi| \geqslant \varepsilon \right) \rightarrow 0.$$
А последнее утверждение "--- это определение сходимости по вероятности.
\end{Proof}

\begin{Wtf}
Совершенно жуткое доказательство, понимается методом вглядывания: если достаточно долго медитировать над каждой импликацией, то все переходы в конце концов станут понятными.
\end{Wtf}

<<<<<<< HEAD
=======
\begin{Th}
Из сходимости в среднем следует сходимость по вероятности.
\end{Th}

\begin{Proof}
Утвеждение теоремы практически сразу же следует из обобщенного неравентва Чебышева:
$$\Pro \left( |\xi_n - \xi| \geqslant \varepsilon \right) \leqslant \frac{\Expec|\xi_n - \xi|^r}{\varepsilon^r}.$$
Переходя в неравенстве к пределу при $n \rightarrow \infty$, получаем требуемое.
\end{Proof}

\begin{Def}
Последовательность случайных величин $\{\xi_n\}$ \uline{слабо сходится} к случайной величине $\xi$, если для любой непрерывной и ограниченной функии последовательность мат. ожиданий функций от $\xi_n$ сходится к мат. ожиданию функции от $\xi$.
$$\{ \xi_n \} \xrightarrow{w} \xi, \text{если} \quad \forall f(x) \colon f(x) \in C \  \text{и} \  |f(x)| \leqslant M \quad \Expec{f(\xi_n)} \rightarrow \Expec{f(\xi)}$$
\end{Def}

\begin{St}
Из сходимости по вероятности следует слабая сходимость.
\end{St}

\begin{Proof}
Пусть $f(x) \colon |f(x)| \leqslant M.$ Также $\forall \varepsilon > 0$ выберем $N$ так, чтобы $\Pro{|\xi| > N} \leqslant \dfrac{\varepsilon}{4M}.$ Выберем $\delta$ так, чтобы $\forall |x| \leqslant N$ и $|x - y| \leqslant \delta$ было выполнено неравенство $|f(x) - f(y)| \leqslant \varepsilon/2$. Тогда:
\begin{multline*}
	\Expec{|f(\xi_n) - f(\xi)|} =
	\Expec{\left(|f(\xi_n) - f(\xi)|; |\xi_n - \xi| \leqslant \delta; |\xi| \leqslant N\right)} + \\
	\Expec{ \left( |f(\xi_n) - f(\xi)|; |\xi_n - \xi| \leqslant \delta; |\xi| > N \right) } +
	\Expec{ \left( |f(\xi_n) - f(\xi)|; |\xi_n - \xi| > \delta) \right) } \leqslant \\
	\varepsilon/2 + \varepsilon/2 + 2M * \Pro(|\xi_n - \xi| > \delta) = \varepsilon + \Pro(|\xi_n - \xi| > \delta).
\end{multline*}
Но из сходимости по веротности следует, что $\Pro(|\xi_n - \xi| > \delta) \rightarrow 0$, значит при $n~\rightarrow~\infty \qquad \Pro(|\xi_n - \xi| > \delta) < \varepsilon$, тогда $\Expec{|f(\xi_n) - f(\xi)|} < 2\varepsilon$, откуда в силу произвольного выбора $\varepsilon$ и в силу того, что модуль числа не превосходит самого числа, получаем требуемое.
\end{Proof}


\begin{Def}
Последовательность слeчайных величин $\{\xi_n\}$ \uline{сходится} к случайной величине $\xi$ \uline{по распределению}, если функции распределения $\{\xi_n\}$ сходятся к функции распределения $\xi$ во всех точках, в которых предельная функция распределения непрерывна.
$$\{ \xi_n \} \xrightarrow{d} \xi, \text{если} \quad F_{\xi_n}(x) \rightarrow F_\xi \quad \forall x \colon F_\xi(x) \in C(x)$$
\end{Def}

\begin{Ex}
Пусть $\{ \xi_n \}$ принимает значения $0$ и $1 - \dfrac{1}{n}$ с вероятностями $\dfrac{1}{2}$. Тогда функция распределения для случайной величины $\xi_i$ выглядит так (Рис.~\ref{ris:distr_seq}): 
\\
\begin{minipage}{0.32\linewidth}
	\begin{tikzpicture}
	\begin{axis}[
		scale=0.5,
		axis x line = middle,
		axis y line = middle,
		xlabel = {$x$},
		ylabel = {$F_{\xi_n}$},
		domain=-0.2:1.2
		ymin=-0.2,
		ymax=1.2,
		extra x ticks={0.75},
		extra x tick style={
			grid=major
		},
		extra x tick label={$\frac{3}{4}$}
	]
	\addplot[blue, domain=-0.2:0] {0};
	\addplot[blue, mark=*] coordinates {(0, 0)};
	\addplot[blue, mark=o] coordinates {
		(0, 0.5)
		(0.75, 0.5)
	};
	\addplot[blue, mark=*] coordinates {(0.75, 0.5)};
	\addplot[blue, mark=none, domain=0.75:1.2] {1};
	\addplot[blue, mark=o] coordinates {(0.75, 1)};
	\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Функция распределения $\xi_n$ при $n = 4$}
	\label{ris:distr_seq}
\end{minipage}
\hfill
\begin{minipage}{0.32\linewidth}
	\begin{tikzpicture}
	\begin{axis}[
		scale=0.5,
		axis x line = middle,
		axis y line = middle,
		xlabel = {$x$},
		ylabel = {$F_{\xi_n}$},
		domain=-0.2:1.2
		ymin=-0.2,
		ymax=1.2
	]
	\addplot[blue, domain=-0.2:0] {0};
	\addplot[blue, mark=*] coordinates {(0, 0)};
	\addplot[blue, mark=o] coordinates {
		(0, 0.5)
		(1, 0.5)
	};
	\addplot[blue, mark=none, domain=1:1.2] {1};
	\addplot[blue, mark=*] coordinates {(1, 1)};
	\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Предельная функция}
	\label{ris:limit_f}
\end{minipage}
\hfill
\begin{minipage}{0.32\linewidth}
	\begin{tikzpicture}
	\begin{axis}[
		scale=0.5,
		axis x line = middle,
		axis y line = middle,
		xlabel = {$x$},
		ylabel = {$F_{\xi_n}$},
		domain=-0.2:1.2
		ymin=-0.2,
		ymax=1.2
	]
	\addplot[blue, domain=-0.2:0] {0};
	\addplot[blue, mark=*] coordinates {(0, 0)};
	\addplot[blue, mark=o] coordinates {
		(0, 0.5)
		(1, 0.5)
	};
	\addplot[blue, mark=*] coordinates {(1, 0.5)};
	\addplot[blue, mark=none, domain=1:1.2] {1};
	\addplot[blue, mark=o] coordinates {(1, 1)};
	\end{axis}
\end{tikzpicture}
\captionof{figure}{Функция распределения $\xi$}
\label{ris:dist_lim}
\end{minipage}
\par\bigskip
Тогда предельная функция будет выглядеть так (Рис.~\ref{ris:limit_f}). Заметим, что она вообще не является функцией распределения, так как в точке $1$ нарушено условие непрерывности слева. Тогда рассмотрим случайную величину $\xi$, принимающую значение $0$ и $1$ с вероятностями $1/2$, тогда ее функция распределения имеет вид (Рис.~\ref{ris:dist_lim}). Заметим, что предельная функция и функция распределения $\xi$ различаются только в точке $1$, в которой $F_\xi(x)$ не является непрерывной, значит $\{\xi_n\} \xrightarrow[]{d} \xi.$
\end{Ex}

\begin{St}
Сходимость по распределению эквивалентна слабой сходимости.
\end{St}

\begin{Proof}
\uline{\textbf{TODO:}} Чет я не знаю, как это доказать =(
\end{Proof}

\begin{St}
Если последовательность случайных величин $\{ \xi_n \}$ сходится по распределению к вырожденной случайной величине $\xi \xequiv{\text{п.н.}} a$, то $\{ \xi_n \}$ сходится к $\xi$ по вероятности.
\end{St}

\begin{Proof}
Ы
\end{Proof}

А теперь докажем теорему, ради которой мы и городили все эти огороды сходимостей. Эта теорема впоследствии будет использоваться при доказательстве центральной предельной теоремы.

\begin{Th}[теорема Леви о непрерывности]
~\\
\begin{enumerate}
	\item Пусть $\{ \xi_n \} \xrightarrow{d} \xi$ \ и \ $f_n(t) := \Expec e^{it\xi_n}$.\ Тогда $\{ f_n(t) \} \rightarrow f(t)$,\ где $f(t) = \Expec e^{it\xi}$.
	\item Пусть теперь $f_n(t) := \Expec e^{it\xi_n}$. Тогда если $f_n(t) \rightarrow f(t) \quad \forall t \in \Real, \quad f(t) \in C(0)$, то $f(t)$ является характеристической функцией некоторой случайной величины $\xi$, такой что $\{ \xi_n \} \xrightarrow{d} \xi.$ 
\end{enumerate}
\end{Th}

\begin{Proof}
\begin{enumerate}
	\item Как было показано выше, сходимость по распределению эквивалентна слабой сходимости. Положим в определении слабой сходимость функции $\phi(x) := Re(e^{itx})$ и $\psi(x) := Im(e^{itx})$, тогда $\Expec\phi(\xi_n) \rightarrow \Expec\phi(\xi)$ и $\Expec\psi(\xi_n) \rightarrow \Expec\psi(\xi)$, откуда следует утверждение теоремы.
	\item Достаточно грустное доказательство, но, если у меня будет настроение, я его напишу. 
\end{enumerate}
\end{Proof}

\begin{Wtf}
А вот картинка, на которой схематично показано, что из чего следует.
\end{Wtf}

\begin{center}
\begin{tikzpicture}
	[
		st/.style={rectangle,draw}
	]
	\node[st] (prob1) {$\{ \xi_n \} \xrightarrow{\text{п.н.}} \xi$};
	\node[rectangle] (empty) [below=of prob1] {~};
	\node[st] (power) [below=of empty] {$\{ \xi_n \} \xrightarrow{(r)} \xi$};
	\node[st] (pro) [right=of empty] {$\{ \xi_n \} \xrightarrow{(\Pro)} \xi$};
	\node[st] (distr) [right=of pro] {$\{ \xi_n \} \xrightarrow{(d)} \xi$};
	\node[st] (weak) [below=of distr] {$\{ \xi_n \} \xrightarrow{(w)} \xi$};	
	\node (pn) at (3.8, -0.2) {$\xi \xequiv{\text{п.н.}} a$};
	\node[st] (th) [right=of distr] {т. о непрерывности};
	
	\draw [->] (prob1) -> (pro);
	\draw [->] (power) -> (pro);
	\draw [->] (pro) -> (distr);
	\draw [<->] (distr) -> (weak);
	\draw [->] (distr) to[bend right=45] (pro);
	\draw [dotted, line width=2pt] (distr) -> (th);
\end{tikzpicture}
\end{center}

\newpage

\section{Характеристические функции}

Введем еще какие-нибудь понятия, которые что-то там нам облегчат (наверное).

\begin{Def}
    Пусть $\xi$ "---~случайная величина. Тогда будем говорить, что $f_\xi \left( x \right) $ "---~\uline{характеристическая функция $\xi$}, если:
    \[
        f_\xi(t) = \Expec e^{it\xi}
    .\] 
\end{Def} 

Поясним раз.  $\Expec e^{it\xi}$, чтобы не сломать мозг раньше времени, понимаем в смысле
$\Expec e^{it\xi} = \Expec \cos{t\xi} + i\Expec \sin{t\xi}$.

Поясним два. Если $\xi$ "---~дискретная случайная величина, то она задается рядом
распределения $\Pro(\xi = x_k)$, тогда  $f_\xi(t) = \sum\limits_{k}^{} e^{itx_k} \Pro(\xi = x_k)$. Если же $\xi$"---~абсолютно непрерывная случайная величина, 
то  $f_\xi(t) = \int\limits_{}^{} e^{itx} dF_\xi(x)$, где $F_\xi(x)$"---~функция распределения $\xi$, а интеграл как всегда берется по всему пространству, да еще и Лебегов.

Прежде чем мы немного поковыряем свойства харфункции и посмотрим примерчики,
введем еще пару определений, ведь без определений так скучно жить!

\begin{Def}
    Функция $f(x)$ называется \uline{неотрицательно определенной}, если:
    \[
    \forall n \in \Nat \quad \forall t_1, \ldots t_n \in \Real \quad
    \forall z_1, \ldots z_n \in \mathbb{C} \quad 
    \sum\limits_{i,j=1}^{n} f(t_i - t_j) z_i \overline{z_j} \geqslant 0
    .\] 
\end{Def} 

\begin{Def}
    Случайная величина $\xi$ имеет \uline{решетчатое распределение}, если
    $\exists a, b \colon \sum\limits_{-\infty}^{+\infty}
    \Pro\left(\xi = a + bk\right)= 1$. Тогда число $b$ называется 
    \uline{шагом распределения}.
\end{Def} 

Свойства (под $f(t)$ подразумевается $f_\xi(t)$ для какой-то случайной величины $\xi$):

\begin{enumerate}
    \item $ \left| f(t) \right| \leqslant 1 $.
    \item $f(0) = 1$.
    \item $f(-t) = \overline{f(t)}$ 
    \item $\forall t \in \Real \quad f_\xi(t) \in \Real \iff$  $\xi$ распределена симметрично (тривиальное следствие из предыдущего).
    \item\label{ravnepr} $f(t)$ равномерно непрерывна на $\Real$.
    \item Если $\eta = a\xi + b$, то $f_\eta(t) = e^{itb}f_\xi(at)$.
    \item\label{nez} Если $\xi_1,  \ldots , \xi_n$"---~независимые случайные величины, и 
    $\eta = \xi_1 +  \ldots + \xi_n$, то $f_\eta(x) = \prod\limits_{k=1}^{n} 
    f_{\xi_k}(x)$.
    \item\label{moments} Если $\Expec \left|\xi^k\right| < \infty$, то 
    $\Expec \xi^k = i^k f^{(k)}_\xi(0)$.
    Если $k$ четно, то верно и обратное утверждение.
    \item \begin{Th}[Бохнера-Хинчина]
            $f(t)$ является характеристической функцией $\iff$ $f(0) = 1$ 
            и  $f(t)$ обладает свойством неотрицательной определенности.
          \end{Th}
    \item $ \left| f_\xi(t) \right|$ интегрируема 
         $\implies p_\xi(x) \xrightarrow[ \left| x \right| \rightarrow \infty]{} 0$,
         где $p_\xi(x)$ "--- плотность распределения.
    \item Случайная величина $\xi$ имеет решетчатое распределение с шагом
        $b \iff \left| f_\xi \left( \dfrac{2\pi}{b} \right) \right| = 1$. 
\end{enumerate}

Не расслабляться! Сейчас докажем некоторые утверждения.

\begin{Proof} \eqref{ravnepr}
\begin{multline*}
    \bigl| f(t + h) - f(t) \bigr| = 
    \left| \int\limits_{-\infty}^{\infty} e^{i(t+h)x}dF(x) + 
    \int\limits_{-\infty}^{\infty} e^{itx}dF(x) \right| \leqslant
    \int\limits_{-\infty}^{\infty} \Bigl| e^{itx} \left( e^{ith} + 1 \right)  \Bigr| dF(x) \leqslant \\
    \leqslant \int\limits_{-\infty}^{\infty} \Bigl| e^{ihx} - 1 \Bigr| dF(x) =
    \underbrace{\int\limits_{|x| \leqslant M}^{} \left| e^{ith} - 1 \right| dF(x)}_{I_1} + 
    \underbrace{\int\limits_{|x| > M}^{} \left| e^{ith}-1 \right|dF(x)}_{I_2} 
.\end{multline*}
Оценим теперь интегралы $I_1$ и $I_2$.
Функция, неепрерывная на компакте равномерно непрерывна на нем, а значит
$\forall \varepsilon > 0 \quad \exists h$, такой что $I_1 < \dfrac{\varepsilon}{2}$.
Для второго интеграла имеем: $ \left| e^{ith} - 1 \right|\leqslant 2 \implies
I_2 \leqslant 2\Pro \left( \left| \xi \right| > M \right) < \dfrac{\varepsilon}{2}$ за счет выбора $M$.
\end{Proof} 

\begin{Proof} \eqref{nez}

Воспользуемся независимостью случайных величин, чтобы разбить одно большое матожидание на много маленьких.
    $$
        f_{\xi_1 +  \ldots + \xi_n} =
        \Expec e^{it(\xi_1 + \ldots + \xi_n)} = 
        \Expec \left( e^{it\xi_1} \ldots e^{it\xi_n} \right) =
        \Expec e^{it\xi_1}  \ldots \Expec e^{it\xi_n} =
        \prod\limits_{k=1}^{n} f_{\xi_k}(x)
    .$$ 
\end{Proof} 

\begin{Proof} \eqref{moments}

    Первую производную посчитаем по определению:
    \[
        f_\xi'(t) = \lim\limits_{h \rightarrow 0} \frac{f_\xi(t + h) - f_\xi(t)}{h}=
        \lim\limits_{h \rightarrow 0} \int\limits_{-\infty}^{\infty} e^{ixt} \frac{e^{ixh} - 1}{h} dF_\xi(x) = \ldots 
    \]
    Тут у нас $\dfrac{e^{ixh} - 1}{h} \leqslant \left| x \right|$,
    поэтому интеграл сходится равномерно (признак Вейерштрасса),
    и мы можем поменять местами предел и интеграл и ничего нам за это не будет:
    \[
        \ldots = \int\limits_{-\infty}^{\infty} e^{ixt} 
        \lim\limits_{h \rightarrow 0} \frac{e^{ixh} - 1}{h} dF_\xi(x) = \ldots
    \] 
    Теперь надо присатльно посмотреть на последнее подынтегральное выражение и заметить там 3-ий замечательный предел:
    \[
        \frac{e^{ixh} - 1}{h} \xrightarrow[h \rightarrow 0]{} ix
    .\] 
    Тогда продолжаем:
    \[
        \ldots = i \int\limits_{-\infty}^{\infty} xe^{itx}dF_\xi(x)
    .\]
    При $t = 0 \  e^{itx} = 1$, поэтому
    \[
        f_\xi'(0) = i \int\limits_{-\infty}^{\infty} xdF(x) = i \Expec \xi
    .\]
    По индукции показываем справедливость для производных старших порядков.
    Обратное утверждение для четных $k$ доказывать не будем. Но там немного полопиталить и доказать это все безобразие по индукции. Можно залезть в Ширяева и удовлетворить свое любопытство.
\end{Proof}

\begin{Why}
    Мяу. Если $\Expec \left| \xi \right|^n < \infty$, то мы можем записать харфункцию в виде суммы:
    \[
        f_\xi(t) = \sum\limits_{k=0}^{n} \frac{t^k}{k!}f_\xi^{(k)}(0) +\overline{o}(t^{k}) =
        1 + \sum\limits_{k=1}^{n} \frac{(it)^k}{k!} \Expec \xi^k + \overline{o}(t^{k}) 
    .\]
    Довольно удобно считать моменты, если мы уверены в их существовании.
\end{Why} 

Теперь рассмотрим некоторые примеры:

\begin{Ex}
    Найдем характеристическую функцию для стандартного нармального распределения $\Norm(0, 1)$
    с плотностью $p(x) = \dfrac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}}$.
    \begin{multline*}
        f(t) = \Expec e^{it\xi} = 
        \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty} e^{itx} e^{\frac{-x^2}{2}} dx =
        \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{\infty} 
        e^{-\frac{1}{2}(x - it)^2} e^{-\frac{t^2}{2}}dx = \\
        = \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \int\limits_{-\infty}^{\infty}  e^{-\frac{1}{2}(x - it)^2}d(x - it) =
        e^{-\frac{t^2}{2}}
    .\end{multline*}
\end{Ex}

\begin{Ex}
    Вот представь ситуацию: входишь ты в хату, а тебе пахан кидает под ноги
    кидает $\cos t^2$ и говорит: <<а найди-ка нам случайную величину, для 
    которой это выражение будет харфункцией>>. Тут главное "--- не зашквариться
    и по понятиям пояснить, что если бы $\cos t^2$ была бы харфункцией, то по
    свойству~\eqref{ravnepr} она была бы равномерно непрерывной на $\Real$,
    а это не так: если $t_1^2 = 2\pi k - \dfrac{\pi}{2}$, а $t_2^2 = 2\pi k$,
    то $\cos t_2^2 - \cos t_1^2 = 1$, а
    \[
        % \vphantom чтобы выровнять знаки корня по высоте
        t_2 - t_1 =\sqrt{\vphantom{\frac{\pi}{2}} 2\pi k} - \sqrt{2\pi k - \frac{\pi}{2} } = 
        \frac{\frac{\pi}{2}}{\sqrt{\vphantom{\frac{\pi}{2}} 2\pi k} + \sqrt{2\pi k - \frac{\pi}{2} }}
        \xrightarrow[k \rightarrow \infty]{} 0 
    .\]
    То есть для любого наперед заданного $\delta > 0$ мы можем найти $k$
    достаточно большое, чтобы разность между $t_1$ и $t_2$ была меньше
    $\delta$, а $\cos t_2^2 - \cos t_1^2 = 1$, что противоречит условию 
    равномерной непрерывности.
\end{Ex}

\begin{Ex}
    А теперь найдем случайную величину $\xi$, такую что $\Expec e^{it\xi} = \cos t$.
    Для этого вспомним, что если $\xi$ "--- дискретная св, то 
    $\Expec g(\xi) = \sum\limits_{k} g(k) \Pro \left( \xi = k \right)  $
    \[
        \cos t = \frac{1}{2} \left( e^{it} + e^{-it} \right)
        \implies \xi = 
        \left\{
            \begin{aligned}
                -1 &, \quad \Pro\left(\xi = -1\right) = \frac{1}{2} \\
                1 &, \quad \Pro \left( \xi = 1 \right) = \frac{1}{2}
            \end{aligned}
        \right. 
    .\] 
\end{Ex} 

\newpage

\section{Закон больших чисел}
В этой части рассмотрим закон больших чисел. Причем сначала скажем, как он формулируется, а потом подоказываем его для различных исходных данных.

\begin{Def}
    Пусть $\{\xi_n\}$ "--- последовательность независимых одинаково распределенных случайных величин, а $S_n = \sum\limits_{k=1}^{n} \xi_n$.
    Говорят, что $\{\xi_n\}$ удовлетворяет \uline{закону больших чисел (ЗБЧ)}, если $\dfrac{S_n}{n} \xrightarrow[]{\Pro} \Expec \xi_i.$ 
\end{Def} 

\begin{Ex}
    Пусть $\xi_k$ независимы и равномерно распределены на отрезке $[-1, 1]$. Тогда матожидание каждого слагаемого равно $0$. Тогда
    \[
        \Pro \left( \left| \dfrac{S_n}{n} \right| \geqslant \varepsilon \right) \leqslant \dfrac{\Disp S_n}{\varepsilon^2} = \dfrac{\Disp \xi_1}{n \varepsilon^2} \rightarrow 0 
    .\]
    То есть $\dfrac{S_n}{n}$ сходится по вероятности к своему матожиданию, значит для данной последовательности слуайных величин выполнен ЗБЧ.
\end{Ex} 

\begin{Th}[ЗБЧ в форме Хинчина]
    Если $\{ \xi_k \}$ "--- независимые одинаково распределенные случайные величины с конечным матождиданием, то для них выполнен ЗБЧ. 
\end{Th} 

\begin{Proof}
    Первое желание, которое может возникнуть при виде данной теоремы "--- записать неравенство Чебышева, но для неравентва Чебышева нужно не только конечное матожидание, но и конечный второй момент, поэтому так не прокатит.
    Тогда мы воспользуемся тем, что если последовательность случайных величин сходится к числу, то сходимость по вероятности эквивалентна слабой сходимости, и в качестве функции в определении слабой сходимости положим характеристическую функцию.
     \[
         f(t) := \Expec e^{it\xi_k} \quad \forall k=\overline{1 \ldots n}  
     .\]
     Здесь нам не важно, для какой случайной величины считать харфункцию или матожидание, так как они все одинаково распределены.
     \[
         f_n(t) := \Expec e^{it\frac{S_n}{n}} = \Expec e^{it \frac{\xi_1 + \ldots + \xi_n}{n}} = \prod\limits_{k=1}^{n} \Expec e^{\frac{it\xi_k}{n}} = 
         f^{n} \left( \frac{t}{n} \right)
    .\]
    Далее вспомним важной свойство харфункции: $a := f'(0) = \Expec \xi_k$. Тогда: 
    \[
        f^{n} \left( \dfrac{t}{n} \right) = \left( 1 + \dfrac{ita}{n} + \overline{o} \left( \dfrac{1}{n} \right)  \right)^{n} \xrightarrow[n \rightarrow \infty ]{} e^{ita} 
    .\]
    Заметим, что $e^{ita}$ непрерывна на всей действительной прямой и обращается в 1 при $t = 0$. Значит по теореме о непрерывности $e^{ita}$ представляет собой харфункцию некоторой случайной величины, к которой слабо сходится $\dfrac{S_n}{n}$. Действительно, эта случайная величина $\xi \xequiv{\text{п.н.}} a$.
    Таким образом, имеем, что $\dfrac{s_n}{n} \xrightarrow[]{w} \Expec \xi_k$,
    но если предельная случайная величина вырождена, то слабая сходимость эквивалентна сходимости по вероятности, значит $\dfrac{s_n}{n} \xrightarrow[]{\Pro} \Expec \xi_k$.
\end{Proof} 

Лулзов ради закинем еще парочку ЗБЧ.

\begin{Th}[ЗБЧ в форме Чебышева]
    Пусть $\{ \xi_k \}$ "--- последовательность некоррелированных случайных величин с равномерно ограниченной дисперсией.
    То есть $\sup\limits_{k \in \mathbb{N}} \Disp \xi_k \leqslant C < \infty$.
    Тогда для данной последовательности случайых величин выполнен ЗБЧ.
\end{Th}

\begin{Proof}
    Последовательно воспользуемся неравенством Чебышева, некоррелированостью случайных величин (а значит дисперсия суммы равна сумме дисперсий) и равномерной ограниченностью дисперсий.
    \[
    \Pro \left( \left| \dfrac{S_n}{n} - \Expec \xi_k \right| \geqslant \varepsilon \right) = 
    \Pro \left( \left| \dfrac{S_n - \Expec S_n}{n} \right| \geqslant \varepsilon \right) \leqslant
    \dfrac{\Disp S_n}{(n\varepsilon)^2} =
    \dfrac{1}{n^2\varepsilon^2} \sum\limits_{k=1}^{n} \Disp \xi_k \leqslant
    \dfrac{C}{n\varepsilon^2} \xrightarrow[n \rightarrow \infty]{} 0
    .\] 
\end{Proof} 

\begin{Note}
    Условия теоремы можно ослабить: вместо некоррелированности потребовать, чтобы сумма ковариаций обращалась в 0, а вместо равномерной ограниченности константой можно потребовать, чтобы дисперсии росли строго медленнее, чем линейная функция.
\end{Note} 

\begin{Th}[Усиленный ЗБЧ (УЗБЧ) в форме Колмогорова]
    Если $\{ \xi_k \}$ "--- независимые одинаково распределенные случайные величины с конечным матождиданием, то $\dfrac{S_n}{n} \xrightarrow[]{\text{п.н.}}  \Expec \xi_k$. 
\end{Th}

\begin{Note}
    Условия те же, что и на ЗБЧ в форме Хинчина, но сходимость уже не по вероятности, а почти наверное.
\end{Note} 

\begin{Proof}
    Все верно. Отвечаю.
\end{Proof} 

\begin{Wtf}
    Собственно, а к чему будут сходиться средние арифметические, если нет матожидания?
\end{Wtf} 

\begin{Ex}
    Распределим $\{ \xi_k \}$ по Коши с параметрами $C(0, \gamma)$ и посмотрим, что будет происходить со средними арифмитическими.
    \[
        p_{\xi_k} (x) = \dfrac{1}{\pi \gamma \left( 1 + \left( \dfrac{x}{\gamma} \right) ^2 \right) }, \quad
        f(t) :=  \Expec e^{it\xi_k} = e^{-\gamma \left| t \right| }
    .\]

    \begin{multline*}
        f_n(t) := \Expec S_n = 
        \Expec e^{it \sum\limits_{k=1}^{n} \xi_k} = 
        \prod\limits_{k=1}^{n} \Expec e^{it\xi_k} = 
        f^{n} \left( t \right) =
        e^{-\gamma n \left| t \right| } \implies \\
        p_{S_n} \left( t \right) = \dfrac{1}{\pi n \gamma \left( 1 + \left( \dfrac{x}{n \gamma} \right)^2 \right) }
    .\end{multline*}
    Теперь предположим, что последовательность $\{ S_n \} $ сходится по вероятности к чему-то, и придем к противоречию.
    Пусть $\Pro \left( \left| \dfrac{S_n}{n} - a \right| \geqslant \varepsilon \right) \rightarrow 0$. Тогда:
    \begin{multline*}
        \Pro \left( \left| \dfrac{S_n}{n} - a \right| \geqslant \varepsilon \right) = \int\limits_{n(a+\varepsilon)}^{+\infty} p(x)dx + \int\limits_{-\infty}^{n(a - \varepsilon)} p(x)dx = 
        \dfrac{1}{\pi} \left. \arctg \left( \dfrac{x}{\gamma n} \right) \right|_{n(a+\varepsilon)}^{+\infty} + 
        \dfrac{1}{\pi} \left. \arctg \left( \dfrac{x}{\gamma n} \right) \right|_{-\infty}^{n(a - \varepsilon)} = \\
        = \dfrac{1}{\pi} \left( \arctg \left( \dfrac{a - \varepsilon}{\gamma} \right) - \arctg \left( \dfrac{a + \varepsilon}{\gamma} \right)  \right) 
        \xcancel{ \xrightarrow[n \rightarrow \infty]{}} \ 0
    .\end{multline*}
    Таким образом, последовательность средних арифметических случайных величин, распределенных по Коши, не сходится по вероятности вообще ни к чему.
\end{Ex} 

\newpage
\section{Центральная предельная теорема}

С места в карьер:

\begin{Def}
    Последовательность случайных величин $\{ \xi_k \}$ \uline{удовлетворяет ЦПТ}, если $\exists \{ a_k \} \in \Real, \{ b_k \} > 0$, такие что 
    \[
        \dfrac{S_k - a_k}{b_k} \xrightarrow[]{d} \xi \sim \Norm(0, 1)
    .\] 
\end{Def}

Разомнемся на чем-нибудь попроще.

\begin{Th}
    Пусть $\{ \xi_k \}$ "--- норсв с конечным матожиданием и конечной ненулевой дисперсией. Тогда $\{ \xi_k \}$ удовлетворяет ЦПТ, причем $a_n = \Expec S_n$,
    a $b_n = \sqrt{\Disp S_n}$, где  $S_n = \sum\limits_{k=1}^{n} \xi_k$.
\end{Th}

\begin{Note}
    Здесь распределение $\dfrac{S_k - \Expec \xi_k}{\sqrt{\Disp \xi_k}}$ сходится к стандартному нормальному не абы как, а очень даже равномерно по $x$ на всей действительной прямой.
\end{Note} 

\begin{Proof}
    Без ограничения общности будем считать, что $\Expec \xi_k = 0$ и  $\Disp \xi_k = 1$.
    Тогда 
     \[
         f(t) := \Expec e^{it\xi_k} = 1 + \cancelto{\text{\scriptsize{0}}}{ita} \quad - \dfrac{t^2}{2} + \overline{o} \left( \dfrac{1}{n} \right) 
    .\]
    \[
        f_{\frac{S_n}{\sqrt{n}}} (t) = \Expec e^{it\frac{S_n}{\sqrt{n}}} = 
        \left( f \left( \dfrac{t}{\sqrt{n}} \right) \right)^{n} = 
        \left( 1 - \dfrac{t^2}{2n} + \overline{o} \left( \dfrac{1}{n} \right)  \right)^{n} 
        \xrightarrow[n \rightarrow \infty]{} e^{-\frac{t^2}{2}}
    .\] 
    Таким образом, харфункция центрированной нормированной случайной величины сходится слабо к харфункции стандартного нормального распределения, а значит, центрированная нормарованная случайная величина сходится по распределению к $\Norm(0, 1)$.
\end{Proof} 

\begin{Why}
    Если вам уже стало плохо, то переживать не стоит "--- дальше будет еще хуже
\end{Why} 

С этого момента положим:
\begin{itemize}
    \item Случайные величины $\{ \xi_k \}$ независимы и определены на одном вероятностном пространстве
    \item $F_k(x) = \Pro (\xi_k < x)$
    \item $S_n = \sum\limits_{k=1}^{n} \xi_k$
    \item $a_k = \Expec \xi_k$, $A_n = \sum\limits_{k=1}^{n} a_k$ 
    \item $b_k^2 = \Disp \xi_k$, $B_n^2 = \sum\limits_{k=1}^{n} b_k^2$
\end{itemize} 

\begin{Th}[Ляпунова]
    Пусть $\mu_k^3 := \Expec \left| \xi_k - a_k \right|^3 < \infty, \quad
    M_n^3 := \sum\limits_{k=1}^{n} \mu_k^3$. Пусть выполнено \uline{условие Ляпунова}:
    \begin{equation}\label{Lyapunov}
        \dfrac{M_n^3}{B_n^3} \xrightarrow[n \rightarrow \infty]{} 0
    \end{equation}
    Тогда $\{ \xi_k \}$ удовлетворяет ЦПТ.
\end{Th}

\begin{Th}[Линдеберга]
    Пусть $\varepsilon > 0$. Тогда если  $a_k < \infty$, $b_k < \infty$ и выполнено \uline{условие Линдеберга}:
    \begin{equation}\label{Lindeberg}
        L_n(\varepsilon) = \dfrac{1}{B_n^2} \sum\limits_{k=1}^{n} 
        \int\limits_{ \left| x - a_k \right| > \varepsilon B_n}^{}
        \left( x - a_k \right)^2 dF_k(x) \xrightarrow[n \rightarrow \infty]{}
        0,
    \end{equation} 
    То $\{ \xi_k \}$ удовлетворяет ЦПТ.
\end{Th} 

\begin{Note}
    Если случайные величины одинаково распределены, то условие Линдеберга эквивалентно существованию дисперсии.
\end{Note}

\begin{Def}
    $\{ \xi_k \} $ удовлетворяют \uline{условию Феллера}, если
    \begin{equation}\label{Feller}
        \forall \varepsilon > 0 \quad
        \max\limits_{1 \leqslant k \leqslant n}
        \Pro \left( \dfrac{ \left| \xi_k - a_k \right| }{B_n} \right) > \varepsilon
        \xrightarrow[n \rightarrow \infty]{} 0. 
    \end{equation}
\end{Def} 

\begin{Th}
\[
    \begin{cases}
        \text{ЦПТ} \\
        (\ref{Feller})
    \end{cases}
    \iff (\ref{Lindeberg}).
\]
\end{Th} 

\begin{Proof}
    Покажем, что из (\ref{Lindeberg}) следует (\ref{Feller}).
    \[    
        \max_{1 \leqslant k \leqslant n} \Pro \left( \frac{ \left| \xi_k - a_k \right|}{B_n} > \varepsilon \right) \leqslant 
        \max_{1 \leqslant k \leqslant n} \frac{b_k^2}{\varepsilon^2 B_n^2} = 
        \max_{1 \leqslant k \leqslant n} \frac{b_k^2}{\varepsilon^2 \left( b_1^2 +  \ldots + b_n^2 \right) }
    \]

    \begin{multline*}
        \max_{1 \leqslant k \leqslant n} \Pro \left( \left| \xi_k - a_k \right| > \varepsilon B_n \right)  \leqslant
        \sum\limits_{k=1}^{n} \Pro \left( \left| \xi_k - a_k \right| > \varepsilon B_n \right) = 
        \sum\limits_{k=1}^{n} \int\limits_{ \left| x - a_k \right| > \varepsilon B_n}^{} 1 dF_k(x) = \\
        \sum\limits_{k=1}^{n} \int\limits_{ \left| x - a_k \right| > \varepsilon B_n}^{} \frac{(x - a_k)^2}{(x - a_k)^2} dF_k(x) \leqslant
        \frac{1}{\varepsilon^2 B_n^2} \int\limits_{ \left| x - a_k \right| > \varepsilon B_n}^{} (x - a_k)^2 dF_k(x) \xrightarrow[n \rightarrow \infty]{} 0.
    \end{multline*} 
    Таким образом, условие Линдеберга влечет за собой условие Феллера.
\end{Proof} 

\begin{Th}
    Условие Ляпунова (\ref{Lyapunov}) влечет за собой условие Линдеберга (\ref{Lindeberg}).
\end{Th}

\begin{Proof}
    \begin{multline*}
        L_n(\varepsilon) = \frac{1}{B_n^2} \sum\limits_{k=1}^{n}  \int\limits_{ \left| x - a_k \right| > \varepsilon B_n}^{} (x - a_k)^2 dF_k(x) = 
        \frac{1}{B_n^2} \sum\limits_{k=1}^{n} \int\limits_{ \left| x - a_k \right| > \varepsilon B_n}^{} \frac{|x - a_k|^3}{|x - a_k|} dF_k(x) \leqslant\\
        \leqslant \frac{1}{\varepsilon B_n^3} \sum\limits_{k=1}^{n} \int\limits_{-\infty}^{+\infty} \left| x - a_k \right|^3 dF_k(x) = \frac{M_n^3}{\varepsilon B_n^3} \xrightarrow[n \rightarrow \infty]{} 0
    .\end{multline*}
\end{Proof} 

\begin{Th}[Неравенство Берри"--~Эссена]
    $\sup\limits_{x} \left| \Pro \left( \dfrac{S_n - A_n}{B_n} < x \right) - \Phi(x) \right| \leqslant C\dfrac{M_n^3}{B_n^3}$. \\
    Если случайные величины одинаково распределены, то правую часть можно переписать так:
    \[
    C \frac{M_n^3}{B_n^3} = C_0 \frac{n \mu_k^3}{n^{3/2} b_k^3} =
    C_0 \frac{\mu_k^3}{b_k^3 \sqrt{n}}
    .\] 
    Константу $C_0$ уточняли, уточняли, уточняли и в конце концов доуточнялись до $C_0 \geqslant \dfrac{\sqrt{10} + 3}{6 \sqrt{2 \pi}}$. А потом доказали, что здесь вообще стоит строгое равенство.
\end{Th} 

>>>>>>> 7dd5d3f64923a3a7dffd71988cf1531076ed4dcd
\end{document}
